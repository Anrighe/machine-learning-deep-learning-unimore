{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Neural Networks\n",
    "\n",
    "Usando i metodi classici di machine learning fissando le feature, sopra una certa quantità di dati non si otterranno miglioramenti. Questo non è vero per le reti neurali: una rete deep, normalmente, all'aumentare dei dati di training continua a migliorare.\n",
    "\n",
    "![Deep Neural Networks1](images/01_Deep_Neural_Networks1.png)\n",
    "\n",
    "Nel caso di un metodo di machine learning tradizionale, all'inizio c'è sempre la fase di ***feature extraction***, che non è collegata alla fase di ottimizzazione del modello in quanto viene fatta prima. Questa cosa però non implica che ci possano essere delle feature migliori.\n",
    "\n",
    "Nel caso del Deep Learning, la parte di ***feature extraction*** e ***apprendimento dei parametri*** sono collegate e vengono quindi fatte insieme. Man mano che fornisco dati di training al sistema cambiano le feature &rarr; il modello di Deep Learning estrae delle feature sempre migliori.\n",
    "\n",
    "La maggior parte dei metodi di machine learning dipende da feature ***hand-crafted***, mentre gli algoritmi di Deep Learning permettono di imparare le migliori feature per il problema che si sta affrontando.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Classifier\n",
    "\n",
    "Operatore che generalizza la logistic regression in un contesto multiclasse. Il softmax per la classe $j$-esima di $z$ è definito come:\n",
    "\n",
    "### $softmax_j(z) = \\frac{e^{z_j}}{\\sum_{k=1}^{K} e^{z_k}}$\n",
    "\n",
    "Il softmax permette di prendere un vettore di score e trasformarlo in un vettore di probabilità che sommano a 1.\n",
    "\n",
    "Per allenare il classificatore viene usa la ***cross-entropy loss***:\n",
    "\n",
    "### $L_i = - \\underset{i}{\\sum} y_i^{true} \\log y^{pred}_i$\n",
    "\n",
    "Nella cross entropy c'è un vettore one not dove c'è un $1$ in corrispondenza della label corretta e poi il vettore delle predizioni per tutte le $k$ classi.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron\n",
    "\n",
    "![Deep Neural Networks2](images/01_Deep_Neural_Networks2.png)\n",
    "\n",
    "Un neurone è costituito da:\n",
    "- un nucleo &rarr; cellula che rappresenta il neurone\n",
    "- dendriti &rarr; connettono il nucleo con altri neuroni\n",
    "- sinapsi &rarr; gelatina che ingloba i dendriti e ha lo scopo di amplificare o sopprimere il segnale che arriva sul dendrito\n",
    "\n",
    "Una volta che il segnale arriva al nucleo, il segnale viene unito con un processo biochimico e poi valuta quanto sia forte il segnale come combinazione di quello dati in ingresso. \n",
    "\n",
    "Se il segnale è abbastanza forte, il neurone si dice <span style=\"color:gold\">***attivo***</span> e invia un segnale di output.\n",
    "\n",
    "Se il segnale è debole, il neurone si dice <span style=\"color:gold\">***spento***</span> e non invia un segnale di output.\n",
    "\n",
    "Da un punto di vista formale, il neurone è rappresentabile nel seguente modo:\n",
    "\n",
    "![Deep Neural Networks3](images/01_Deep_Neural_Networks3.png)\n",
    "\n",
    "- Ho una serie di ingressi $x_0$, $x_1$, $x_2$, ... che rappresentano gli input che arrivano da altri neuroni.\n",
    "- La sinapsi la rappresento con un peso $w_0$, $w_1$, $w_2$, ... che rappresenta quanto il segnale che arriva da un neurone è importante per il neurone in questione.\n",
    "- Internamente al nucleo i segnali sono raccolti e sommati\n",
    "- Per decidere se un neurone sarà attivo o spento utilizzo una funzione di soglia &rarr; ***activation function*** &rarr; ad esempio la ***sigmoide***.\n",
    "\n",
    "Se cercassimo i $w$ usando la binary cross entropy, avremmo una logistic regression a due classi. Se invece avessimo 10 neuroni, uno per ogni classe con al posto della sigmoide la softmax e l'allenassimo con la cross entropy si avrebbe la logistic regression multiclasse.\n",
    "\n",
    "Quindi un neurone o uno strato di neuroni, è assimilabile a una logistic regression a due classi se usa la sigmoide o a n-classi se sono tanti neuroni con la softmax.\n",
    "\n",
    "Anche il singolo neurone verrà allenato con il gradient descent. Il neurone ha un input $x$ e un output $y$ che è la sigmoide di $w^T x$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuron Activation\n",
    "\n",
    "### Sigmoide\n",
    "\n",
    "![Deep Neural Networks4](images/01_Deep_Neural_Networks4.png)\n",
    "\n",
    "### $\\sigma(x) = \\frac{1}{1 + e^{-x}}$\n",
    "\n",
    "Se uso come funzione di attivazione una sigmoide, il neurone diventa esattamente una logistic regression a due classi. Ha come output un valore tra $0$ e $1$ che rappresenta la probabilità che l'input appartenga alla classe $1$.\n",
    "\n",
    "La sigmoide ha due problematiche:\n",
    "- L'output non è centrato sullo zero: ovvero se ho 0 in ingresso, l'uscita è 0.5\n",
    "- Satura e uccide il gradiente: per tutti gli score maggiori di 4 (circa) la sigmoide mi dà 1 in uscita, quindi quell'elemento non mi cambierà mai i parametri della logistic regression perché sto classificando correttamente. Da un certo punto in poi il gradiente satura, ovvero tutte le uscite della mia funzione di attivazione sono uguali indipendentemente dall'ingresso. Se l'input sarebbe 500 (quindi molto appartenente a una classe), la sigmoide lo riconoscerebbe al pari dell'input 4.\n",
    "\n",
    "### Tanh\n",
    "\n",
    "![Deep Neural Networks5](images/01_Deep_Neural_Networks5.png)\n",
    "\n",
    "La tanh è una sigmoide che è centrata sullo zero. Ha come output un valore tra $-1$ e $1$ che rappresenta la probabilità che l'input appartenga alla classe $1$.\n",
    "\n",
    "### $tanh(x) = 2\\sigma(x) - 1$\n",
    "\n",
    "Anche questa funzione ha lo stesso problema della sigmoide: satura e uccide il gradiente.\n",
    "\n",
    "### ReLU\n",
    "\n",
    "![Deep Neural Networks6](images/01_Deep_Neural_Networks6.png)\n",
    "\n",
    "La ReLU è una funzione che ha come output $0$ se l'input è negativo e $x$ se l'input è positivo.\n",
    "\n",
    "### $ReLU(x) = \\max(0, x)$\n",
    "\n",
    "Questa non satura mai il gradiente, ma non mi restituisce in uscita una probabilità.\n",
    "\n",
    "### Perché non usare una funzione di attivazione lineare?\n",
    "\n",
    "Composizione di funzioni lineari fanno sempre una funzione lineare. Senza non linearità, le reti neurali si ridurrebbero a una logistic regression a un layer.\n",
    "\n",
    "Supponiamo di avere la seguente funzione ($\\phi$ rappresenta le non linearità):\n",
    "\n",
    "### $f(x) = \\phi(W_2 \\phi(W_1 x))$\n",
    "\n",
    "Se ci sbarazziamo delle non linearità, questa si riduce a:\n",
    "\n",
    "### $f(x) = W_2 W_1 x = W x$\n",
    "\n",
    "che è chiaramente ancora lineare.\n",
    "\n",
    "Se metto la funzione non lineare in mezzo, le matrici $W$ non si possono moltiplicare\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "Una rete neurale è una combinazione di neuroni organizzati in strati dove l'output di un neurone dello stato precedente diventa l'input di un neurone dello stato successivo.\n",
    "\n",
    "![Deep Neural Networks7](images/01_Deep_Neural_Networks7.png)\n",
    "&rarr; Questa rete ha 47 parametri.\n",
    "\n",
    "Il primo strato è chiamato <span style=\"color:gold\">***input layer***</span>, l'ultimo <span style=\"color:gold\">***output layer***</span> e gli strati intermedi <span style=\"color:gold\">***hidden layers***</span>.\n",
    "\n",
    "Di solito ci sono tanti neuroni quante le feature date in ingresso al sistema.\n",
    "\n",
    "Negli strati nascosti, il numero di neuroni è un iperparametro, quindi decido io quanti metterne.\n",
    "\n",
    "Normalmente ho tanti neuroni di output quante sono le classi del mio problema, tranne nel caso in cui ho due classi perché se riesco a esprimere l'output in maniera probabilistica mi basta un neurone solo.\n",
    "\n",
    "Ogni neurone in uscita mi dà lo score di una determinata classe.\n",
    "\n",
    "Da un punto di vista matriciale, la rete in foto ha la seguente equazione:\n",
    "\n",
    "### $out = \\phi(W_3 \\phi(W_2 \\phi(W_1 x)))$\n",
    "\n",
    "dove:\n",
    "\n",
    "- $\\phi$ è la funzione di attivazione\n",
    "- $x \\in \\real^6$ è il vettore di input\n",
    "- $W_1 \\in \\real^{4 \\times 6}$ è la matrice dei pesi del primo strato: dove 6 sono le feature d'ingresso e 4 sono i valori in uscita\n",
    "- $W_2 \\in \\real^{3 \\times 4}$ è la matrice dei pesi del secondo strato: 4 valori d'ingresso e 3 valori in uscita\n",
    "- $W_3 \\in \\real^{1 \\times 3}$ è la matrice dei pesi del terzo strato: 3 valori d'ingresso e 1 valore in uscita\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forward Propagation\n",
    "\n",
    "La Forward Propagation è il processo con cui metto dei valori in ingresso e guardo lo score di uscita.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Representational Power\n",
    "\n",
    "Le reti neurali con almeno uno strato nascosto può rappresentare a meno di $\\epsilon$ qualsiasi funzione continua da un insieme compatto a $\\real^n$:\n",
    "\n",
    "### $\\forall x, |f(x) - g(x)| < \\epsilon$\n",
    "\n",
    "Questo vuol dire che le reti neurali sono degli ***Universal function approximators***. Questo è vero da un punto di vista teorico, ma nella pratica non è così. \n",
    "\n",
    "Tuttavia, più strati metto, più la rete neurale è in grado di approssimare una funzione. (Per questo motivo le reti si chiamano deep, perché hanno tanti strati).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting Hyperparameters\n",
    "\n",
    "Una rete ha una serie di iperparametri:\n",
    "- Numero di strati\n",
    "- Quanti neuroni per strato\n",
    "  \n",
    "***Più neuroni metto***, più aumenta la capacità della mia rete di ***rappresentare funzioni complesse*** e più la mia rete è soggetta all'***overfitting***.\n",
    "\n",
    "All'aumentare degli strati interni la rete fa delle separazioni molto più complesse ma tende a fare overfitting.\n",
    "\n",
    "![Deep Neural Networks8](images/01_Deep_Neural_Networks8.png)\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
