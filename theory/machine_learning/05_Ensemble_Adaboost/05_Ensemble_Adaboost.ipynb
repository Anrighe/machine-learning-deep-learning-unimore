{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensebles\n",
    "\n",
    "Un ensemble è una collezione di classificatori, tutti addestrati per performare lo stesso compito.\n",
    "\n",
    "Un ensemble può consistere in diverse versioni dello stesso modello, o di modelli diversi.\n",
    "\n",
    "L'output finale per un classificatore ensemble è tipicamente ottenere attraverso una media (pesata) delle predictions dei singoli modelli dell'ensemble.\n",
    "\n",
    "Un ensemble di diversi modelli che conseguono una simile performance generalizzativa, spesso superano in performance i risultati dei singoli modelli.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble intuition\n",
    "\n",
    "Supponiamo di avere un ensemble di funzioni di classificatori binari $f_k(x)$ per $k = 1,...,K$.\n",
    "\n",
    "Supponiamo inoltre che mediamente abbiano lo stesso errore di classificazione $\\epsilon = E_{p(x,y)}[y \\neq f_k(x)] < 0.5$, ma che gli errori che commettono siano indipendenti.\n",
    "\n",
    "L'intuizione sta nel fatto che la maggioranza dei $K$ classificatori, dovrebbe classificare correttamente un dato esempio, dove un singolo classificatore potrebbe fallire.\n",
    "\n",
    "Un semplice voto di maggioranza potrebbe migliorare la performance di classificazione diminuendo la varianza.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proof of Majority Voting\n",
    "\n",
    "Supponiamo di avere un ensemble di $K$ classificatori binari, dove ogni classificatore ha una accuracy $\\alpha \\gt 0.5$.\n",
    "\n",
    "La maggioranza dei votanti commette un errore quando $\\frac{K}{2} + 1$ classificatori sbagliano.\n",
    "\n",
    "L'errore segue la distribuzione binomiale:\n",
    "\n",
    "#### $P(x \\leq k) = \\sum_{i=0}^{k} \\binom{K}{k} \\alpha^i (1-\\alpha)^{K-i}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Esempio\n",
    "\n",
    "Si suppone di avere 5 classificatori con accuracy $\\alpha = 0.75$. L'errore di probabilità dell'ensemble è: (l'ensemble sbaglia quando almeno 3 classificatori sbaglianom da questo $x \\leq 2$)\n",
    "\n",
    "### $P(x \\leq 2) = \\sum_{i=0}^{2} \\binom{5}{i} 0.75^i (0.25)^{5-i} = 0.105$\n",
    "\n",
    "L'accuratezza dell'ensemble è $1-P(x \\leq 2) = 0.895$.\n",
    "\n",
    "<span style=\"color:gold;\">Questo è vero quando gli errori tra i classificatori non sono correlati</span>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Independent Training Sets\n",
    "\n",
    "Supponiamo di collezionare diversi training set indipendenti $T_{r1}, ..., T_{rK}$ e di usarli per addestrare diverse istanze dello stesso classificatore ottenendo $K$ funzioni di classificazione $f_1(x), ..., f_K(x)$. Quindi traning set indipendenti si traduce dati diversi che descrivono lo stesso problema.\n",
    "\n",
    "I classificatori addestrati in questo modo sono garantiti di commettere errori indipendenti sui dati di test.\n",
    "\n",
    "Se l'errore atteso di ogni classificatore è minore di 0.5, allora il voto di maggioranza pesato è garantito di ridurre l'errore di generalizzazione atteso.\n",
    "\n",
    "<span style=\"color:gold;\">Qual è il punto debole di questo approccio?</span>\n",
    "- Se devo costruire diversi training set che sono indipendenti l'un l'altro e descrivono lo stesso problema sto sicuramente perdendo informazioni. Inoltre non ho la garanzia che allenando i classificatori su questi training set indipendenti, questi performino bene nel mondo reale.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging\n",
    "\n",
    "Bootstrap aggregation o <span style=\"color:gold;\">Bagging</span> è un'approssimazione al metodo precedente che prende un singolo training set $T_r$ e ne prende K sottocampioni casuali (con rimpiazzo) per formare K training set $T_{r1}, ..., T_{rK}$.\n",
    "\n",
    "Ogni training set viene usato per addestrare una diversa istanza dello stesso classificatore ottenendo $K$ funzioni di classificazione $f_1(x), ..., f_K(x)$.\n",
    "\n",
    "Si gioca sul fatto che prendendo un campione casuale, la probabilità di avere lo stesso esempio su ogni dataset è molto bassa.\n",
    "\n",
    "Con questo metodo uso tutto il dataset e ogni classificatore ha la stessa probabilità di vedere ogni esempio.\n",
    "\n",
    "Come contro si ha che gli errori non saranno totalmente indipendenti perchè i training set non sono indipendenti, ma il campionamento casuale di solito introduce abbastanza diversità per diminuire la varianza e dare una performance migliore.\n",
    "\n",
    "![Ensemble_Adaboost1](./images/Ensemble_Adaboost1.png)\n",
    "\n",
    "Può succedere che un elemento sia presente più di una volta nello stesso dataset. <- non ci sono garanzie da questo punto di vista\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bagging proof\n",
    "\n",
    "Supponiamo che ogni classificatore impari una funzione $y = f_i(x) + e_i$ dove $e$ è l'errore.\n",
    "\n",
    "L'errore medio di $M$ classificatori sul dataset $D$ è:\n",
    "\n",
    "### $Ɛ_{AV} = \\frac{1}{M} \\sum_{i=1}^{M} E_D[e_i]$\n",
    "\n",
    "Dove $E_D[e_i]$ è l'expected value sul dataset $D$ dell'errore $e_i$.\n",
    "\n",
    "La predizione bagged è:\n",
    "\n",
    "### $y_{bagged} = \\frac{1}{M} \\sum_{i=1}^{M} f_i(x) + e_i(x)$\n",
    "\n",
    "Dove $f_i(x) + e_i(x)$ è la combinazione delle risposte di ogni singolo classificatore\n",
    "\n",
    "L'errore medio del classificatore bagged è:\n",
    "\n",
    "### $Ɛ = E_D[\\frac{1}{M} \\sum_{i=1}^{M} e_i(x)]$\n",
    "\n",
    "Che è l'expected value sul world dataset del valore medio di $e$ per il singolo elemento $x$\n",
    "\n",
    "La differenza tra $Ɛ_{AV} = \\frac{1}{M} \\sum_{i=1}^{M} E_D[e_i]$, e $Ɛ = E_D[\\frac{1}{M} \\sum_{i=1}^{M} e_i(x)]$ è che il primo prendendo $E_d[E_i]$ è il \"negato\" dell'accuracy del classificatore (ovvero 1-accuracy), mentre il secondo, ogni errore conta per ogniclassificatore che ho considerato, mentre ogni errore nel primo viene contato come 1.<br>\n",
    "Quindi tipicamente se gli errori non sono correlati:\n",
    "\n",
    "$Ɛ = E_D[\\frac{1}{M} \\sum_{i=1}^{M} e_i(x)]$ $\\leq$ $Ɛ_{AV} = \\frac{1}{M} \\sum_{i=1}^{M} E_D[e_i]$\n",
    "\n",
    "È uguale solo nel caso in cui tutti i classificatori commettono lo stesso errore, altrimenti nel secondo caso sto contando un errore come $\\frac{1}{\\frac{1}{M}}$, mentre nel primo caso solo come $\\frac{1}{M}$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting\n",
    "\n",
    "L'idea è quella di combinare i classificatori non in parallelo ma in serie.\n",
    "\n",
    "Vogliamo che un classificatore che fa parte della serie di classificatore sia bravo a risolvere solo una porzione del problema.\n",
    "\n",
    "L'idea è quella di sfruttare una catena dove ogni classificatore sa qualcosa riguardo i classificatori precedenti. Inoltre un classificatore nella catena, conosce anche gli elementi nei quali il classificatore precedente commette una predizione errata.\n",
    "\n",
    "![Ensemble_Adaboost2](./images/Ensemble_Adaboost2.png)\n",
    "\n",
    "Ogni volta che qualche canale sbaglia a predirre il meteo, il suo valore di fiducia diminuisce.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo Boosting\n",
    "\n",
    "Il boosting parte da un dataset $D$ e addestra sequenzialmente classificatori uguali $f_i$ focalizzandosi sugli errori dei classificatori precedenti.\n",
    "\n",
    "Assegna ad ogni elemento del mio dataset $D$ $x_k $ pesi uguali $w_k = \\frac{1}{N}$\n",
    "\n",
    "Itera con $i$ da 1 a $M$ numero classificatori che ho nella catena:\n",
    "1. Prendi un nuovo dataset $D_i$ da $D$ usando i pesi $\\{w_i^k\\}^N_{k=1}$ come probabilità di sampling per ogni record $x$\n",
    "2. Addestra il classificatore $f_i$ su $D_i$ e misura l'accuratezza e registrala come $\\alpha_i$\n",
    "3. Se $x_k$ è stato classificato erroneamente aumenta il suo peso al prossimo stage $w_{i+1}$ e ricalcola i pesi\n",
    "\n",
    "La regola di decisione finale è $y(x_{new}) = sign(\\sum_{i}^{M} \\alpha_i f_i(x_{new}))$\n",
    "\n",
    "Ovvero il segno  (positivo o negativo) della somma del prodotto tra la fiducia e la risposta del singolo classificatore\n",
    "\n",
    "![Ensemble_Adaboost3](./images/Ensemble_Adaboost3.png)\n",
    "\n",
    "Il boosting allena sequenzialmente i classificatori aumentando l'importanza degli elementi classificati erroneamente.\n",
    "\n",
    "Ogni classificatore nella catena deve avere un'accuratezza maggiore di 0.5.\n",
    "\n",
    "Agli elementi classificati non correttamente viene fornito un peso maggiore ed è più probabile che siano campionati pian piano che la catena aumenta.\n",
    "\n",
    "Catene più lunghe tendono ad aumentare l'accuratezza ma anche il rischio di overfitting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from a Discrete Distribution Example\n",
    "\n",
    "Un possibile modo per campionare una distribuzione discreta è usando l'algoritmo <span style=\"color:gold;\">cdf</span>.\n",
    "\n",
    "***Input***: una distribuzione discreta $H(x)$ con $x \\in \\{1, ..., N\\}$ con $N$ numero di bin e $K$ numero di campioni da generare.\n",
    "\n",
    "***Output***: $K$ indici.\n",
    "\n",
    "***Step 1***: costruire la ***cdf table*** $T$:\n",
    "\n",
    "$T_x = [\\sum_0^{x-1} H(x), \\sum_0^x (x)H(x)]$\n",
    "\n",
    "Che è una matrice con due elementi per riga, dove ci sono tante righe quanti gli elementi nel dataset. Il primo numero è la somma dell'istogramma di tutti gli elementi precedenti, mentre il secondo è la somma dell'istogramma di tutti gli elementi precedenti più l'istogramma dell'elemento corrente.\n",
    "\n",
    "***Step 2***: Draw indexes:\n",
    "\n",
    "```\n",
    "while num iter = K do:<br>\n",
    "    pesco un numero casuale $n \\in [0,1]$<br>\n",
    "    controllo in quale intervallo è contenuto $n$ -> find $k | T_k[0] \\leq n \\lt T_k[1]$<br>\n",
    "    output $k$\n",
    "end\n",
    "```\n",
    "\n",
    "![Ensemble_Adaboost4](./images/Ensemble_Adaboost4.png)\n",
    "\n",
    "Il campionamento può essere molto lento perché devo aggiornare la table ogni volta!\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adaboost\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
