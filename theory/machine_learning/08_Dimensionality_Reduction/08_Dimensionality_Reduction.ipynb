{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Redcution\n",
    "\n",
    "I metodi di riduzione delle dimensioni cercando di ridurre il numero di feature con cui è rappresentato un elemento senza perdere capacità descrittiva (es risolvo il problma iniziale che aveva elementi da 1000 feature con elementi da 100 feature).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional Scaling (MDS)\n",
    "\n",
    "Parto da un dataset iniziale in cui ogni punto è rappresentato con le sue $K$ feature originali. Voglio avere una nuova rappresentazione (in uno spazio di dimensione $d$) in cui ogni punto è rappresentato con le sue $d$ feature e voglio minimizzare lo <span style=\"color:gold;\">stress</span>:\n",
    "\n",
    "## $stress = \\sqrt{\\frac{\\sum_{i,j} (d_{ij} - \\hat{d}_{ij})^2}{\\sum_{i,j} d_{ij}^2}} , d_{ij} = | o_j - o_i | $ e $ \\overset{\\frown}{d_{ij}} = | \\overset{\\frown}{o}_j - \\overset{\\frown}{o}_i | $\n",
    "\n",
    "Dove lo stress è la distanza tra i punti nel nuovo spazio e la distanza tra i punti nel vecchio spazio, ovvero quanto le distanze variano. \n",
    "\n",
    "- $o$ è la rappresentazione dello spazio originale\n",
    "- $o_j - o_i$ è la distanza tra l'elemento i-esimo e l'elemento j-esimo nello spazio ***originale***\n",
    "- $\\overset{\\frown}{o}$ è la rappresentazione dello spazio ridotto\n",
    "- $\\overset{\\frown}{o}_j - \\overset{\\frown}{o}_i$ è la distanza tra l'elemento i-esimo e l'elemento j-esimo nello spazio ***ridotto***\n",
    "- Lo stress si calcola come somma della differenza di ogni coppia di elmeneti $i$, $j$ rispettivamente della distanza trasformata e di quella originale diviso una costante di normalizzazione.\n",
    "\n",
    "Io vorrei avere un nuovo spazio in cui le distanze tra i punti siano le stesse, ma non è possibile. Quindi cerco di minimizzare lo stress, ovvero cerco di minimizzare la differenza tra le distanze nello spazio originale e quelle nello spazio ridotto.\n",
    "\n",
    "Si risolve in modo euristico per mezzo dello <span style=\"color:gold;\">Steepest Descent</span>:\n",
    "- Parto da una rappresentazione iniziale (assegno delle coordinate a caso a tutti i punti)\n",
    "- Valuta lo stress\n",
    "- Prende un punto a caso, lo muove nella direzione che diminuisce lo stress\n",
    "- Ripeti fino a quando lo stress non è minore di una soglia\n",
    "\n",
    "Questa è un'euristica perché non ho la garanzia che muovendo un punto non mi allontani dalla mia soluzione &rarr; no garanzia di convergenza\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "Tutte le volte che si riducono le dimensioni di un dataset, si introduce il concetto di embedding.\n",
    "\n",
    "Supponendo di avere una matrice delle distanze D che misura la distanza tra tutti gli elementi del dataset: ***il concetto di embedding implica la necessità di incorporare i miei punti in uno spazio a dimensione minore in modo tale che le distanze tra due punti dello spazio originale sia più vicino possibile alla distanza tra i due punti nello spazio ridotto***:\n",
    "\n",
    "### $D(i,j) \\approx D^{'}(i,j)$\n",
    "\n",
    "Questo tipo di embedding si chiama <span style=\"color:gold;\">isometrico</span> cioè un embedding che conserva le distanze:\n",
    "- $D^{'}(F(i),F(j)) = D(i,j)$\n",
    "\n",
    "Un altro tipo di embedding è il <span style=\"color:gold;\">contrattivo</span> cioè un embedding che non conserva le distanze:\n",
    "- $D^{'}(F(i),F(j)) \\leq D(i,j)$\n",
    "\n",
    "A seconda di come viene realizzata questa trasformazione dei punti, gli embedding si dividono in:\n",
    "- ***Lineari***: i punti vengono proiettati in uno spazio di dimensione minore da una **trasformazione lineare** (tendenzialmente prodotto riga per colonna)\n",
    "- ***Non lineari***: i punti sono proiettati su un nuovo spazio in modo non lineare \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "Il ***PCA*** è un metodo lineare di riduzione delle dimensioni che segue il seguente algoritmo:\n",
    "\n",
    "1. Creo una matrice $X$ di dimensione $N \\times d$ dove ogni riga è un vettore $x_n$ che rappresenta un punto del dataset\n",
    "2. Sottraggo la media $x$ (il vettore medio di tutto il dataset) a ogni riga di $X$ per centrare il dataset rispetto all'origine del feature-space dei punti &rarr: serve per centrare tutti gli elementi del dataset verso l'origine (in un caso euclideo). Lo faccio calcolando la media per colonna del mio dataset e sottraendo a tutti gli elementi del vettore che contiene il valor medio per colonne il valore medio di tutto il dataset.\n",
    "3. Calcolo la matrice di covarianza (covarianza è la versione multidimensionale della varianza che si calcola con $(x - \\mu)^2$) $\\Sigma$ di $X$: se io ho più di una feature posso avere $x_i - \\mu_i$ * $x_j - \\mu_j$ che mi dice come varia la feature $i$-esima rispetto alla feature $j$-esima e per questo motivo si chiama covarianza. La matrice di covarianza si calcola facendo un prodotto riga per colonna tra la matrice $X$ e la sua trasposta $X^T$ e dividendo per il numero di elementi del dataset $N$. \n",
    "\n",
    "    Dato un dataset $n \\times d$, la covarianza sarà:\n",
    "\n",
    "    ## $c_ij = \\underset{D}{\\sum} \\frac{(x_i - \\mu_i) *  (x_j - \\mu_j)}{n}$\n",
    "\n",
    "    Ovvero la covarianza tra la feature $i$-esima e la feature $j$-esima è la somma di tutti i prodotti tra le differenze tra i valori della feature $i$-esima e la media della feature $i$-esima e le differenze tra i valori della feature $j$-esima e la media della feature $j$-esima diviso il numero di elementi del dataset. \n",
    "\n",
    "    La matrice di covarianza ottenuta sarà $d \\times d$.\n",
    "\n",
    "    Sulla diagonale ci sarà la varianza di ogni singola feature e nei termini fuori dalla diagonale avrò le coviarianze tra i termini delle singole feature (la matrice della covarianza è sempre simmetrica).\n",
    "\n",
    "    La matrice di covarianza è indicata anche con $\\Sigma$ e il suo calcolo è esprimibile anche come:\n",
    "    \n",
    "    ## $\\Sigma = \\frac{D^T * D}{n}$\n",
    "\n",
    "    Dove $D$ è la matrice $N \\times d$ centrata rispetto alla media.\n",
    "\n",
    "    - <span style=\"color:gold;\">Più la covarianza di due feature è alta, più significa che al variare di una varia anche l'altra</span>. \n",
    "    \n",
    "    - <span style=\"color:gold;\">Se la covarianza è bassa, quando varia una l'altra varia poco rispetto al suo valor medio</span> (idealmente la covarianza è 0 se una feature non cambia mai).\n",
    "\n",
    "    ***Se la covarianza di una feature vale 0 allora significa che è inutile e non la devo considerare perché non è in relazione con una determinata feature***.\n",
    "\n",
    "    A me interessano le feature con covarianza maggiore perché se i miei punti hanno varianza bassa intorno a un asse, vuol dire che i miei punto sono molto vicini tra di loro &rarr; per dividere in classi è più conveniente che i punti siano sparpagliati. Questo suggerisce che un alto valore di varianza intorno a una determinata feature dice che intorno a quell'asse/feature i punti sono molto più sparpagliati, mentre gli assi in cui ho un valore di varianza basso sono gli assi in cui i punti sono molto vicini tra di loro e quindi farò fatica a dividerli.\n",
    "\n",
    "\n",
    "4. Calcolo gli autovettori e gli autovalori della matrice di covarianza $\\Sigma$. Gli autovalori della matrice di covarianza sono ordinati in modo ***decrescente*** perché gli autovalori sono collegati alla varianza per il corrispettivo autovettore.\n",
    "   Se io ho una matrice $d \\times d$ ho $d$ autovalori: ho $d$ assi e l'autovalore è la variana lungo quel determinato asse \n",
    "   \n",
    "   &rarr; ottengo tanti assi ordinati per varianza dei punti lungo il relativo asse. \n",
    "   \n",
    "   Il primo autovettore (quello a varianza più alta) è quello in cui i punti si sparpagliano di più.\n",
    "   L'ultimo autovettore è l'asse in cui i punti sono più vicini tra di loro.\n",
    "\n",
    "5. Se decido che il nuovo spazio deve avere $M$ feature, prendo $M$ autovettori con gli autovalori più grandi e costruisco una matrice $M \\times d$ che è una matrice di proiezione che proietta linearmente un punto bidimensionale su $ numeri, che sono le $M$ nuove feature che ottengo proiettando il mio punto nello spazio trasformato.\n",
    "\n",
    "La PCA va a cercare gli assi in cui i punti si sparpagliano di più, quindi se devo tenere alcune feature terrò quelle, perché sarà molto più facile dividere i punti in un problema di classificazione, mentre quelle in cui i punti sono molto vicini perché rendono più difficile la classificazione.\n",
    "\n",
    "In termini di operazioni dello spazio, la PCA ruota gli assi e sceglie quali tenere. Questa operazione di rotazione viene fatta dalla matrice $d \\times M$ costituita dagli $M$ autovettori con gli $M$ autovalori più grandi.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretazione geometrica della PCA\n",
    "\n",
    "Supponendo di avere una serie di punti:\n",
    "\n",
    "![Dimensionality Reduction 1](./images/08_Dimensionality_Reduction1.png)\n",
    "\n",
    "La PCA cerca di trovare gli assi in cui i punti si sparpagliano di più e quindi sostituisce i seguenti assi a quelli originali:\n",
    "\n",
    "![Dimensionality Reduction 2](./images/08_Dimensionality_Reduction2.png)\n",
    "\n",
    "Lungo l'asse <b><span style=\"color:red;\">rosso</span></b> i punti sono molto sparpagliati (l'autovettore con autovalore più grande). \n",
    "\n",
    "Proiettare lungo l'asse <b><span style=\"color:red;\">rosso</span></b> permette di dividere i punti molto meglio rispetto all'asse <b><span style=\"color:blue;\">blu</span></b>.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quante componenti scegliere?\n",
    "\n",
    "Posso scegliere un numero di feature ottime attraverso un'euristica.\n",
    "\n",
    "Se prendo gli autovalori (quindi le varianze) e faccio in modo che sommino a 1, posso scegliere un numero di feature che mi spiega una certa percentuale della varianza totale.\n",
    "\n",
    "Posso considerare come contenuto informativo una certa quantità di un autovalore e quindi scegliere un numero di feature che mi spiega una certa percentuale della varianza totale.\n",
    "\n",
    "Esempio: se tengo solo il primo autovettore e il suo autovalore normalizzato fa $0.7$ mi dice che sto conservando il $70\\%$ delle informazioni che avevo nel dataset originale, ecc..\n",
    "\n",
    "Normalmente si cerca di conservare dall'$80\\%$ all'$90\\%$ delle informazioni del dataset originale.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problemi e limitazioni della PCA\n",
    "\n",
    "La PCA lavora su matrici di covarianza che sono $d \\times d$ e quindi se ho un dataset con $1000$ feature, la matrice di covarianza sarà $1000 \\times 1000$ e quindi devo calcolare $1000^2$ elementi &rarr; scala molto male\n",
    "\n",
    "È molto inefficiente applicare la PCA nel caso in cui il numero di feature superi il numero di elementi.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplacian Eigenmaps\n",
    "\n",
    "Il ***Laplacian Eigenmaps*** è un metodo non lineare di riduzione delle dimensioni: costruisce una trasformazione non lineare tra lo spazio di partenza e lo spazio di arrivo in modo tale che si conservino le distanze.\n",
    "\n",
    "### Manifold\n",
    "\n",
    "Si tratta di uno spazio che è localmente euclideo, ovvero preso un punto, nel suo intorno posso approssimare la superficie con un piano euclideo. \n",
    "\n",
    "Esempio: la terra è un geoide, ma se prendo un punto e lo approssimo con un piano, posso approssimare la superficie della terra con un piano.\n",
    "\n",
    "### Embedding\n",
    "\n",
    "L'embedding è una trasformazione che mappa un punto da uno spazio di partenza a uno spazio di arrivo, il quale possiede meno feature o dimensioni rispetto allo spazio di partenza (vogliamo però che una proprietà si conservi).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Manifold and Dimensionality Reduction\n",
    "\n",
    "Voglio sempre ridurre le dimensioni, però voglio farlo su un manifold, ovvero voglio ridurre le dimensioni in modo tale che le distanze tra i punti si conservino, perché quando ho un vettore di feature che descrive un punto non so com'è fatto lo spazio dove si trova quel punto &rarr; ***potrebbe non essere globalmente euclideo***, però sarà localmente euclideo: i punti vicini ad esso posso approssimarli come se fossero su un piano centrato nel punto che ho scelto.\n",
    "\n",
    "Posso quindi approssimare lo spazio di cui non conosco la geometria con un manifold, ovvero uno spazio che è localmente euclideo.\n",
    "\n",
    "Voglio quindi proiettare un certo punto $y$ dello spazio originale su un manifold in un certo punto, cercando di preservare le distanze euclidee del punto da tutti quelli che lo circondano **anche nello spazio di destinazione**.:\n",
    "\n",
    "![Dimensionality Reduction 3](./images/08_Dimensionality_Reduction3.png)\n",
    "\n",
    "Il problema è che su un manifold, non considerando le distanze locali non sono euclidee:\n",
    "\n",
    "![Dimensionality Reduction 4](./images/08_Dimensionality_Reduction4.png)\n",
    "\n",
    "In verde è rappresentata la ***Distanza Geodesica***, ovvero la distanza che si percorre lungo la superficie del manifold per andare da un punto all'altro, che è diversa da quella euclidea.\n",
    "\n",
    "Non sapendo com'è fatto un manifold, devo sfruttare la proprietà che dice che un manifold è localmente euclideo: quello che posso fare è approssimare la curvatura del manifold come tanti piccoli piani:\n",
    "- dato un punto, cerco il punto più vicino a esso e approssimo la distanza come se fosse euclidea\n",
    "- poi metto un nuovo piano nel nuovo punto e ripeto il procedimento approssimando la curva con tante piccole distanze locali euclidee\n",
    "\n",
    "Tramite la conservazione delle distanze di un punto con i suoi vicini, sto conservando indirettamente anche le distanze geodesiche.\n",
    "\n",
    "### LLE and Laplacian Eigenmaps\n",
    "\n",
    "Tre step per l'algoritmo basato su grafi:\n",
    "\n",
    "1. Costruisco il grafo, dove ogni punto è collegato ai suoi $K$ (iperparametro) punti più vicini in termini di distanza euclidea (sto commettendo ovviamente un errore di approssimazione: più un punto è lontano e meno la distanza euclidea approssima quella geodesica, però scegliendo i $K$ punti più vicini sto lavorando solo in un intorno locale e quindi l'errore di approssimazione dovrenbbe essere piccolo)\n",
    "2. Costruisco la matrice di adiacenza del grafo che rappresenta come i punti sono distribuiti nello spazio\n",
    "3. Trova un embedding, ovvero un set di coordinate che preserva le prorprietà che ho incorporato nella matrice di adiacenza\n",
    "\n",
    "### Procedimento\n",
    "\n",
    "Considero uno spazio $M$ dimensionale e dei punti $x_i \\in M$. Voglio trovare un nuovo set di coordinate $y_1, ..., y_n \\in R^m$, dove $m$ è molto minore del numero di feature originale e in modo che $y_i$ rappresenti $x_i$\n",
    "\n",
    "### Costruzione del grafo\n",
    "\n",
    "Il grafo di adiacenza contiene per ogni punto un $1$ soltanto se la distanza tra due punti è ***minore*** di una certa soglia, oppure un $1$ soltanto per i suoi $K$ vicini più vicini: versione della matrice di adiacenza ***Simple Minded***.\n",
    "\n",
    "![Dimensionality Reduction 5](./images/08_Dimensionality_Reduction5.png)\n",
    "\n",
    "Questi collegamenti mi dicono che i punti sono vicini tra di loro e quindi la distanza euclidea approssima bene la distanza geodesica.\n",
    "\n",
    "Un'altra alernativa consiste di non mettere $1$ o $0$ nella matrice di adiacenza, ma mettere la similarità tra i due:\n",
    "\n",
    "### Heat kernel $(t \\in \\real)$: $A_{ij} = e^{-\\frac{||x_i - x_j||^2}{t}}$\n",
    "\n",
    "Quindi collego i punti solo se la loro similarità è maggiore di una certa soglia, solo che al posto di mettere 1, metto la similarità tra i due punti.\n",
    "\n",
    "$t$, o *temperatura* è un iperparametro e più è alto, più le distanze si schiacciano verso l'essere tutte uguali. Se la temperatura è bassa (minore di 1), amplifico le differenze.\n",
    "\n",
    "\n",
    "Successivamente dovrò trovare una sottomatrice $Y$ di dimensione $n \\times m$, dove $m$ è il numero di feature che voglio nello spazio di destinazione che approssima la matrice di adiacenza $A$ e in cui minimizzo l'energia di Dirichlet:\n",
    "\n",
    "### $arg \\underset{Y}{min} trace (Y^' L Y)$\n",
    "\n",
    "Dove $L$ è il Laplaciano del mio grafo che è $D-A$, dove $D$ è la matrice diagonale che contiene la somma di tutti i valori di ogni riga sulla diagonale e $A$ è la matrice di adiacenza.\n",
    "\n",
    "La soluzione di questo problema sono i primi $m$ autovettori della matrice $L$.\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
