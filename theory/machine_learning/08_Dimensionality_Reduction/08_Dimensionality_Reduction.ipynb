{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality Redcution\n",
    "\n",
    "I metodi di riduzione delle dimensioni cercando di ridurre il numero di feature con cui è rappresentato un elemento senza perdere capacità descrittiva (es risolvo il problma iniziale che aveva elementi da 1000 feature con elementi da 100 feature).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional Scaling (MDS)\n",
    "\n",
    "Parto da un dataset iniziale in cui ogni punto è rappresentato con le sue $K$ feature originali. Voglio avere una nuova rappresentazione (in uno spazio di dimensione $d$) in cui ogni punto è rappresentato con le sue $d$ feature e voglio minimizzare lo <span style=\"color:gold;\">stress</span>:\n",
    "\n",
    "## $stress = \\sqrt{\\frac{\\sum_{i,j} (d_{ij} - \\hat{d}_{ij})^2}{\\sum_{i,j} d_{ij}^2}} , d_{ij} = | o_j - o_i | $ e $ \\overset{\\frown}{d_{ij}} = | \\overset{\\frown}{o}_j - \\overset{\\frown}{o}_i | $\n",
    "\n",
    "Dove lo stress è la distanza tra i punti nel nuovo spazio e la distanza tra i punti nel vecchio spazio, ovvero quanto le distanze variano. \n",
    "\n",
    "- $o$ è la rappresentazione dello spazio originale\n",
    "- $o_j - o_i$ è la distanza tra l'elemento i-esimo e l'elemento j-esimo nello spazio ***originale***\n",
    "- $\\overset{\\frown}{o}$ è la rappresentazione dello spazio ridotto\n",
    "- $\\overset{\\frown}{o}_j - \\overset{\\frown}{o}_i$ è la distanza tra l'elemento i-esimo e l'elemento j-esimo nello spazio ***ridotto***\n",
    "- Lo stress si calcola come somma della differenza di ogni coppia di elmeneti $i$, $j$ rispettivamente della distanza trasformata e di quella originale diviso una costante di normalizzazione.\n",
    "\n",
    "Io vorrei avere un nuovo spazio in cui le distanze tra i punti siano le stesse, ma non è possibile. Quindi cerco di minimizzare lo stress, ovvero cerco di minimizzare la differenza tra le distanze nello spazio originale e quelle nello spazio ridotto.\n",
    "\n",
    "Si risolve in modo euristico per mezzo dello <span style=\"color:gold;\">Steepest Descent</span>:\n",
    "- Parto da una rappresentazione iniziale (assegno delle coordinate a caso a tutti i punti)\n",
    "- Valuta lo stress\n",
    "- Prende un punto a caso, lo muove nella direzione che diminuisce lo stress\n",
    "- Ripeti fino a quando lo stress non è minore di una soglia\n",
    "\n",
    "Questa è un'euristica perché non ho la garanzia che muovendo un punto non mi allontani dalla mia soluzione &rarr; no garanzia di convergenza\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embeddings\n",
    "\n",
    "Tutte le volte che si riducono le dimensioni di un dataset, si introduce il concetto di embedding.\n",
    "\n",
    "Supponendo di avere una matrice delle distanze D che misura la distanza tra tutti gli elementi del dataset: ***il concetto di embedding implica la necessità di incorporare i miei punti in uno spazio a dimensione minore in modo tale che le distanze tra due punti dello spazio originale sia più vicino possibile alla distanza tra i due punti nello spazio ridotto***:\n",
    "\n",
    "### $D(i,j) \\approx D^{'}(i,j)$\n",
    "\n",
    "Questo tipo di embedding si chiama <span style=\"color:gold;\">isometrico</span> cioè un embedding che conserva le distanze:\n",
    "- $D^{'}(F(i),F(j)) = D(i,j)$\n",
    "\n",
    "Un altro tipo di embedding è il <span style=\"color:gold;\">contrattivo</span> cioè un embedding che non conserva le distanze:\n",
    "- $D^{'}(F(i),F(j)) \\leq D(i,j)$\n",
    "\n",
    "A seconda di come viene realizzata questa trasformazione dei punti, gli embedding si dividono in:\n",
    "- ***Lineari***: i punti vengono proiettati in uno spazio di dimensione minore da una **trasformazione lineare** (tendenzialmente prodotto riga per colonna)\n",
    "- ***Non lineari***: i punti sono proiettati su un nuovo spazio in modo non lineare \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA\n",
    "\n",
    "Il ***PCA*** è un metodo lineare di riduzione delle dimensioni che segue il seguente algoritmo:\n",
    "\n",
    "1. Creo una matrice $X$ di dimensione $N \\times d$ dove ogni riga è un vettore $x_n$ che rappresenta un punto del dataset\n",
    "2. Sottraggo la media $x$ (il vettore medio di tutto il dataset) a ogni riga di $X$ per centrare il dataset rispetto all'origine del feature-space dei punti &rarr: serve per centrare tutti gli elementi del dataset verso l'origine (in un caso euclideo). Lo faccio calcolando la media per colonna del mio dataset e sottraendo a tutti gli elementi del vettore che contiene il valor medio per colonne il valore medio di tutto il dataset.\n",
    "3. Calcolo la matrice di covarianza (covarianza è la versione multidimensionale della varianza che si calcola con $(x - \\mu)^2$) $\\Sigma$ di $X$: se io ho più di una feature posso avere $x_i - \\mu_i$ * $x_j - \\mu_j$ che mi dice come varia la feature $i$-esima rispetto alla feature $j$-esima e per questo motivo si chiama covarianza. La matrice di covarianza si calcola facendo un prodotto riga per colonna tra la matrice $X$ e la sua trasposta $X^T$ e dividendo per il numero di elementi del dataset $N$. \n",
    "\n",
    "    Dato un dataset $n \\times d$, la covarianza sarà:\n",
    "\n",
    "    ## $c_ij = \\underset{D}{\\sum} \\frac{(x_i - \\mu_i) *  (x_j - \\mu_j)}{n}$\n",
    "\n",
    "    Ovvero la covarianza tra la feature $i$-esima e la feature $j$-esima è la somma di tutti i prodotti tra le differenze tra i valori della feature $i$-esima e la media della feature $i$-esima e le differenze tra i valori della feature $j$-esima e la media della feature $j$-esima diviso il numero di elementi del dataset. \n",
    "\n",
    "    La matrice di covarianza ottenuta sarà $d \\times d$.\n",
    "\n",
    "    Sulla diagonale ci sarà la varianza di ogni singola feature e nei termini fuori dalla diagonale avrò le coviarianze tra i termini delle singole feature (la matrice della covarianza è sempre simmetrica).\n",
    "\n",
    "    La matrice di covarianza è indicata anche con $\\Sigma$ e il suo calcolo è esprimibile anche come:\n",
    "    \n",
    "    ## $\\Sigma = \\frac{D^T * D}{n}$\n",
    "\n",
    "    Dove $D$ è la matrice $N \\times d$ centrata rispetto alla media.\n",
    "\n",
    "    - <span style=\"color:gold;\">Più la covarianza di due feature è alta, più significa che al variare di una varia anche l'altra</span>. \n",
    "    \n",
    "    - <span style=\"color:gold;\">Se la covarianza è bassa, quando varia una l'altra varia poco rispetto al suo valor medio</span> (idealmente la covarianza è 0 se una feature non cambia mai).\n",
    "\n",
    "    ***Se la covarianza di una feature vale 0 allora significa che è inutile e non la devo considerare perché non è in relazione con una determinata feature***.\n",
    "\n",
    "    A me interessano le feature con covarianza maggiore perché se i miei punti hanno varianza bassa intorno a un asse, vuol dire che i miei punto sono molto vicini tra di loro &rarr; per dividere in classi è più conveniente che i punti siano sparpagliati. Questo suggerisce che un alto valore di varianza intorno a una determinata feature dice che intorno a quell'asse/feature i punti sono molto più sparpagliati, mentre gli assi in cui ho un valore di varianza basso sono gli assi in cui i punti sono molto vicini tra di loro e quindi farò fatica a dividerli.\n",
    "\n",
    "\n",
    "4. Calcolo gli autovettori e gli autovalori della matrice di covarianza $\\Sigma$. Gli autovalori della matrice di covarianza sono ordinati in modo ***decrescente*** perché gli autovalori sono collegati alla varianza per il corrispettivo autovettore.\n",
    "   Se io ho una matrice $d \\times d$ ho $d$ autovalori: ho $d$ assi e l'autovalore è la variana lungo quel determinato asse \n",
    "   \n",
    "   &rarr; ottengo tanti assi ordinati per varianza dei punti lungo il relativo asse. \n",
    "   \n",
    "   Il primo autovettore (quello a varianza più alta) è quello in cui i punti si sparpagliano di più.\n",
    "   L'ultimo autovettore è l'asse in cui i punti sono più vicini tra di loro.\n",
    "\n",
    "5. Se decido che il nuovo spazio deve avere $M$ feature, prendo $M$ autovettori con gli autovalori più grandi e costruisco una matrice $M \\times d$ che è una matrice di proiezione che proietta linearmente un punto bidimensionale su $ numeri, che sono le $M$ nuove feature che ottengo proiettando il mio punto nello spazio trasformato.\n",
    "\n",
    "La PCA va a cercare gli assi in cui i punti si sparpagliano di più, quindi se devo tenere alcune feature terrò quelle, perché sarà molto più facile dividere i punti in un problema di classificazione, mentre quelle in cui i punti sono molto vicini perché rendono più difficile la classificazione.\n",
    "\n",
    "In termini di operazioni dello spazio, la PCA ruota gli assi e sceglie quali tenere. Questa operazione di rotazione viene fatta dalla matrice $d \\times M$ costituita dagli $M$ autovettori con gli $M$ autovalori più grandi.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretazione geometrica della PCA\n",
    "\n",
    "Supponendo di avere una serie di punti:\n",
    "\n",
    "![Dimensionality Reduction 1](./images/08_Dimensionality_Reduction1.png)\n",
    "\n",
    "La PCA cerca di trovare gli assi in cui i punti si sparpagliano di più e quindi sostituisce i seguenti assi a quelli originali:\n",
    "\n",
    "![Dimensionality Reduction 2](./images/08_Dimensionality_Reduction2.png)\n",
    "\n",
    "Lungo l'asse <b><span style=\"color:red;\">rosso</span></b> i punti sono molto sparpagliati (l'autovettore con autovalore più grande). \n",
    "\n",
    "Proiettare lungo l'asse <b><span style=\"color:red;\">rosso</span></b> permette di dividere i punti molto meglio rispetto all'asse <b><span style=\"color:blue;\">blu</span></b>.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quante componenti scegliere?\n",
    "\n",
    "Posso scegliere un numero di feature ottime attraverso un'euristica.\n",
    "\n",
    "Se prendo gli autovalori (quindi le varianze) e faccio in modo che sommino a 1, posso scegliere un numero di feature che mi spiega una certa percentuale della varianza totale.\n",
    "\n",
    "Posso considerare come contenuto informativo una certa quantità di un autovalore e quindi scegliere un numero di feature che mi spiega una certa percentuale della varianza totale.\n",
    "\n",
    "Esempio: se tengo solo il primo autovettore e il suo autovalore normalizzato fa $0.7$ mi dice che sto conservando il $70\\%$ delle informazioni che avevo nel dataset originale, ecc..\n",
    "\n",
    "Normalmente si cerca di conservare dall'$80\\%$ all'$90\\%$ delle informazioni del dataset originale.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problemi e limitazioni della PCA\n",
    "\n",
    "La PCA lavora su matrici di covarianza che sono $d \\times d$ e quindi se ho un dataset con $1000$ feature, la matrice di covarianza sarà $1000 \\times 1000$ e quindi devo calcolare $1000^2$ elementi &rarr; scala molto male\n",
    "\n",
    "È molto inefficiente applicare la PCA nel caso in cui il numero di feature superi il numero di elementi.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplacian Eigenmaps\n",
    "\n",
    "Il ***Laplacian Eigenmaps*** è un metodo non lineare di riduzione delle dimensioni "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
