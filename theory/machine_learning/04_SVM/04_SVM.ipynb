{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ***<span style=\"color:gold;\">classificazione binaria</span>*** può essere vista come il compito di separare le classi in uno spazio di feature.\n",
    "\n",
    "![SVM](images/svm1.png)\n",
    "\n",
    "Posso scegliere molte rette per dividere questi punti:\n",
    "\n",
    "![SVM](images/svm2.png)\n",
    "\n",
    "Mi piacerebbe scegliere quella retta che non passa esattamente vicino a uno di questi punti, bensì una che sia il più lontana possibile dai punti, perché mi permette di costruire una \"banda di sicurezza\" tra le due classi\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Margin\n",
    "Il <span style=\"color:gold;\">margine</span> è lo spazio intorno alla retta di separazione in cui non cadono punti. Per semplicità il margine viene sempre costruito simmetrico\n",
    "\n",
    "Per costruirlo utilizzo l'equazione della distanza punto-retta:\n",
    "\n",
    "### $r_i = \\frac{y_i(w^Tx_i + b)}{||w||}$\n",
    "\n",
    "dove $x_i$ è il punto in cui devo calcolare la distanza.\n",
    "\n",
    "Quindi io voglio trovare la retta in cui se io faccio la distanza punto-retta tra i punti più vicini alla retta, essa è massima.\n",
    "\n",
    "![SVM](images/svm3.png)\n",
    "\n",
    "Non tutti i punti sono uguali e concorrono allo stesso modo a scegliere la retta migliore, ma solamente quelli più vicini alla retta. <br>\n",
    "Essi si chiamano anche <span style=\"color:gold;\">vettori di supporto</span>.\n",
    "\n",
    "---\n",
    "\n",
    "Dato un dataset $D = (x_i, y_i)_{i\\in[1..N]}$, con la label $y_i \\in {+1, -1}$ e con $\\rho$ come margine, vogliamo:\n",
    "\n",
    "- #### se $y_i = +1$ allora $w^Tx_i + b \\geq \\rho$, ovvero che tutti i punti che stanno sopra la retta ci stiano almeno per $\\frac{\\rho}{2}$\n",
    "- #### se $y_i = -1$ allora $w^Tx_i + b \\leq -\\rho$, ovvero che tutti i punti che stanno sotto alla retta ci stiano almeno per $\\frac{\\rho}{2}$\n",
    "\n",
    "Posso scrivere queste due disequazioni in un'equazione unica (giocando sul fatto che le label sono $+1$ e $-1$):\n",
    "\n",
    "### $y_i(w^Tx_i + b) \\geq \\rho,   \\forall x_i \\in D$\n",
    "\n",
    "Per tutti i vettori di supporto (quelli cerchiati), questa disuguaglianza in realtà è un'uguaglianza:\n",
    "\n",
    "### $w^Tx_i + b = \\frac {\\rho}{2}$\n",
    "\n",
    "Che riscrivo come:\n",
    "\n",
    "### $r_s = \\frac{y_i(w^Tx_i + b)}{||w||} = \\frac{\\rho}{2||w||}$\n",
    "\n",
    "Prendo il vettore $w$ che non conosco, e divido la quantità per $\\frac {\\rho}{2}$, e lo chiamo $\\hat w$ ottenendo:\n",
    "\n",
    "###  $\\frac{\\rho}{2||w||} = \\frac {1}{||\\hat w||}$\n",
    "\n",
    "Quindi il mio margine è 2 volte $r_s$, ovvero (da questo momento verranno usati $w$ e $\\hat w$ in modo intercambiabile dato che è il set di parametri che voglio trovare):\n",
    "\n",
    "### $\\rho = 2r_s = \\frac{2}{||w||} = \\frac{2}{||\\hat w||}$\n",
    "\n",
    "Il mio classificatore dovrà quindi risolvere una disequazione tale per cui dovrà massimizzare $\\frac{2}{||w||}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Quadratic Problem\n",
    "\n",
    "Formula di massimizzazione:\n",
    "\n",
    "### $argmax_{w,b} \\frac{2}{||w||}$\n",
    "\n",
    "Con i vincoli:\n",
    "\n",
    "### $y_i(w^Tx_i + b) \\geq 1,  \\forall (x_i, y_i) \\in D$\n",
    "\n",
    "dove posso scrivere $\\geq 1$ perché ho diviso tutto per $\\rho/2$:\n",
    "\n",
    "![SVM](images/svm4.png)\n",
    "\n",
    "Quindi voglio trovare $w$ e $b$ tali per cui il margine è massimo, ma che allo stesso tempo soddisfino i vincoli.\n",
    "\n",
    "Dato che $\\frac{2}{||w||}$ è massimo quando $||w||$ è minimo, posso riscrivere il problema come:\n",
    "\n",
    "### $argmin_{w,b} ||w||^2$\n",
    "\n",
    "Con i vincoli:\n",
    "\n",
    "### $y_i(w^Tx_i + b) \\geq 1,  \\forall (x_i, y_i) \\in D$\n",
    "\n",
    "Siccome la radice nella norma è problematica, elevo tutto al quadrato.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QP Solvers\n",
    "\n",
    "In generale, i problemi di minimizzazione quadratica soggetti a vincoli lineari (<span style=\"color:gold;\">QP</span>) vengono risolti da software &rarr; <span style=\"color:gold;\">QP solver</span>. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Optimization (ottimizzazione vincolata)\n",
    "\n",
    "Normalmente, un problema di ottimizzazione con vincoli di disuguaglianza si imposta come:\n",
    "\n",
    "### $min f(x)$ <br>\n",
    "### soggetta ai vincoli $h_k(x) \\geq 0, for k \\in 1, ..., K$\n",
    "\n",
    "Devo incorporare i miei vincoli nella equazione da risolvere utilizzando i moltiplicatori di Lagrange, ottenendo una funzione Lagrangiana in cui ho ancora la mia funzione che voglio risolvere $f(x)$ - la sommatoria di un moltiplicatore, uno per ogni vincolo, moltiplicato per il vincolo stesso:\n",
    "\n",
    "### $L(x, \\mu) = f(x) - \\sum_{k=1}^K \\alpha_i h_k(x), \\forall \\alpha_i \\geq 0$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
