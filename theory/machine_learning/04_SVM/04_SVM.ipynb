{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ***<span style=\"color:gold;\">classificazione binaria</span>*** può essere vista come il compito di separare le classi in uno spazio di feature.\n",
    "\n",
    "![SVM1](./images/SVM1.png)\n",
    "\n",
    "\n",
    "Posso scegliere molte rette per dividere questi punti:\n",
    "\n",
    "![SVM2](./images/SVM2.png)\n",
    "\n",
    "Mi piacerebbe scegliere quella retta che non passa esattamente vicino a uno di questi punti, bensì una che sia il più lontana possibile dai punti, perché mi permette di costruire una \"banda di sicurezza\" tra le due classi\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Margin\n",
    "Il <span style=\"color:gold;\">margine</span> è lo spazio intorno alla retta di separazione in cui non cadono punti. Per semplicità il margine viene sempre costruito simmetrico\n",
    "\n",
    "Per costruirlo utilizzo l'equazione della distanza punto-retta:\n",
    "\n",
    "### $r_i = \\frac{y_i(w^Tx_i + b)}{||w||}$\n",
    "\n",
    "dove $x_i$ è il punto in cui devo calcolare la distanza.\n",
    "\n",
    "Quindi io voglio trovare la retta in cui se io faccio la distanza punto-retta tra i punti più vicini alla retta, essa è massima.\n",
    "\n",
    "![SVM3](./images/SVM3.png)\n",
    "\n",
    "Non tutti i punti sono uguali e concorrono allo stesso modo a scegliere la retta migliore, ma solamente quelli più vicini alla retta. <br>\n",
    "Essi si chiamano anche <span style=\"color:gold;\">vettori di supporto</span>.\n",
    "\n",
    "---\n",
    "\n",
    "Dato un dataset $D = (x_i, y_i)_{i\\in[1..N]}$, con la label $y_i \\in {+1, -1}$ e con $\\rho$ come margine, vogliamo:\n",
    "\n",
    "- #### se $y_i = +1$ allora $w^Tx_i + b \\geq \\rho$, ovvero che tutti i punti che stanno sopra la retta ci stiano almeno per $\\frac{\\rho}{2}$\n",
    "- #### se $y_i = -1$ allora $w^Tx_i + b \\leq -\\rho$, ovvero che tutti i punti che stanno sotto alla retta ci stiano almeno per $\\frac{\\rho}{2}$\n",
    "\n",
    "Posso scrivere queste due disequazioni in un'equazione unica (giocando sul fatto che le label sono $+1$ e $-1$):\n",
    "\n",
    "### $y_i(w^Tx_i + b) \\geq \\rho,   \\forall x_i \\in D$\n",
    "\n",
    "Per tutti i vettori di supporto (quelli cerchiati), questa disuguaglianza in realtà è un'uguaglianza:\n",
    "\n",
    "### $w^Tx_i + b = \\frac {\\rho}{2}$\n",
    "\n",
    "Che riscrivo come:\n",
    "\n",
    "### $r_s = \\frac{y_i(w^Tx_i + b)}{||w||} = \\frac{\\rho}{2||w||}$\n",
    "\n",
    "Prendo il vettore $w$ che non conosco, e divido la quantità per $\\frac {\\rho}{2}$, e lo chiamo $\\hat w$ ottenendo:\n",
    "\n",
    "###  $\\frac{\\rho}{2||w||} = \\frac {1}{||\\hat w||}$\n",
    "\n",
    "Quindi il mio margine è 2 volte $r_s$, ovvero (da questo momento verranno usati $w$ e $\\hat w$ in modo intercambiabile dato che è il set di parametri che voglio trovare):\n",
    "\n",
    "### $\\rho = 2r_s = \\frac{2}{||w||} = \\frac{2}{||\\hat w||}$\n",
    "\n",
    "Il mio classificatore dovrà quindi risolvere una disequazione tale per cui dovrà massimizzare $\\frac{2}{||w||}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Quadratic Problem\n",
    "\n",
    "Formula di massimizzazione:\n",
    "\n",
    "### $argmax_{w,b} \\frac{2}{||w||}$\n",
    "\n",
    "Con i vincoli:\n",
    "\n",
    "### $y_i(w^Tx_i + b) \\geq 1,  \\forall (x_i, y_i) \\in D$\n",
    "\n",
    "dove posso scrivere $\\geq 1$ perché ho diviso tutto per $\\rho/2$:\n",
    "\n",
    "![SVM4](./images/SVM4.png)\n",
    "\n",
    "Quindi voglio trovare $w$ e $b$ tali per cui il margine è massimo, ma che allo stesso tempo soddisfino i vincoli.\n",
    "\n",
    "Dato che $\\frac{2}{||w||}$ è massimo quando $||w||$ è minimo, posso riscrivere il problema come:\n",
    "\n",
    "### $argmin_{w,b} ||w||^2$\n",
    "\n",
    "Con i vincoli:\n",
    "\n",
    "### $y_i(w^Tx_i + b) \\geq 1,  \\forall (x_i, y_i) \\in D$\n",
    "\n",
    "Siccome la radice nella norma è problematica, elevo tutto al quadrato.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QP Solvers\n",
    "\n",
    "In generale, i problemi di minimizzazione quadratica soggetti a vincoli lineari (<span style=\"color:gold;\">QP</span>) vengono risolti da software &rarr; <span style=\"color:gold;\">QP solver</span>. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Optimization (ottimizzazione vincolata)\n",
    "\n",
    "Normalmente, un problema di ottimizzazione con vincoli di disuguaglianza si imposta come:\n",
    "\n",
    "### $min f(x)$ <br>\n",
    "### soggetta ai vincoli $h_k(x) \\geq 0, for k \\in 1, ..., K$\n",
    "\n",
    "Devo incorporare i miei vincoli nella equazione da risolvere utilizzando i moltiplicatori di Lagrange, ottenendo una funzione Lagrangiana in cui ho ancora la mia funzione che voglio risolvere $f(x)$ - la sommatoria di un moltiplicatore, uno per ogni vincolo, moltiplicato per il vincolo stesso:\n",
    "\n",
    "### $L(x, \\mu) = f(x) - \\sum_{k=1}^K \\alpha_i h_k(x), \\forall \\alpha_i \\geq 0$\n",
    "\n",
    "dove $\\alpha_i$ sono i moltiplicatori di Lagrange, ovvero un numero o positivo o nullo che moltiplica il vincolo.\n",
    "\n",
    "Ci possono essere due casi:\n",
    "- In tutti i casi in cui \"qua dentro ci si mette\" $x_i$ e il vincolo è soddisfatto, il moltiplicatore di Lagrange varrà 0 (quel vincolo è già soddisfatto e non devo considerarlo), il vincolo viene considerato inattivo e la soluzione del problema sarebbe la $x$ che annulla la derivata.\n",
    "- Se però il vincolo non è soddisfatto e vale l'uguaglianza, cioè $x_i$ annulla il mio vincolo ($h_i(x_i) = 0$) allora il moltiplicatore di Lagrande deve essere attivo ($\\alpha > 0$) e quindi la soluzione sarà quella che annulla le derivate - il vettore dei moltiplicatori di Lagrange per le derivate dei vincoli:\n",
    "\n",
    "![SVM5](./images/SVM5.png)<br>Quindi deve annullare sia la derivata della funzione che la derivata del vincolo.\n",
    "\n",
    "Il triangolo verso il basso indica un prodotto riga per colonna(?).\n",
    "\n",
    "Spiegazione:\n",
    "\n",
    "![SVM6](./images/SVM6.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Optimization\n",
    "\n",
    "Mettendo tutto insieme si ottiene otteniamo le <span style=\"color:gold;\">KKT</span>, ovvero le condizioni necessarie affinché il nostro problema vincolato sia risolvibile.\n",
    "\n",
    "La prima condizione indica che la soluzione annulli questa equazione (di fatto la derivata):<br>\n",
    "### $\\nabla f(x*) - \\alpha^T \\nabla h(x*) = 0$\n",
    "\n",
    "La seconda condizione era vera fin dall'inizio, cioè dato che $\\alpha_i$ sono moltiplicatori di Lagrange possono essere solo positivi o nulli:<br>\n",
    "### $\\alpha_i \\geq 0$\n",
    "\n",
    "La terza condizione ci dice che se uno dei vincoli non è nullo il suo moltiplicatore dev'essere 0, e viceversa se h è nullo, il suo moltiplicatore dev'essere positivo:<br>\n",
    "### $\\alpha^T h(x) = 0$\n",
    "\n",
    "La quarta condizione dice che la mia soluzione deve comunque rispettare i vincoli (questo succederà per costruzione)\n",
    "### $h(x*) \\leq 0$\n",
    "\n",
    "\n",
    "Se il vincolo è una funzione affine, e sostanzialmente lineare, $f$ è convessa e $g$ è differenziabile in \"modo infinito\", le ***KKT*** sono sia condizioni necessarie che sufficienti, quindi se sono verificate sono sicuro che ci sia una soluzione.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM Separable Case\n",
    "\n",
    "Quindi, nel caso dell'SVM il mio problema è questo:\n",
    "\n",
    "### $argmin_{w,b} ||w||^2$\n",
    "\n",
    "Dove voglio minimizzare $||w||^2$ soggetta a questi vincoli:\n",
    "\n",
    "### $y_i(w^Tx_i + b) \\geq 1,  \\forall (x_i, y_i) \\in D$\n",
    "\n",
    "Con $D$ composto da $N$ elementi, la funzione Lagrangiana diventa:\n",
    "\n",
    "### $min_{w,b,\\alpha} L(w,b,\\alpha) = \\frac{1}{2}||w||^2 - \\sum_{i=1}^N \\alpha_i(y_i(w^Tx_i + b) - 1)$\n",
    "\n",
    "Che ha come KKT $\\forall i = 1 .. N$:\n",
    "\n",
    "### $\\alpha_i \\geq 0$\n",
    "\n",
    "### $y_i(w^Tx_i+b)-1 \\geq 0$\n",
    "\n",
    "### $\\alpha_i(y_i(w^Tx_i+b)-1) = 0$\n",
    "\n",
    "Per trovare il minimo posso derivare e porre uguale a 0, e dato che dipende da 3 variabili dovrò derivare 3 volte.\n",
    "\n",
    "Inizio con la derivata rispetto a $w$ ottengo:\n",
    "\n",
    "### $w = \\sum_{i=1}^N \\alpha_i y_i x_i$\n",
    "\n",
    "Che è l'equazione più importante perché mette in relazione $w$ al moltiplicatore, in quanto posso riscriverlo in funzione di $\\alpha$ &rarr; relazione tra primale e duale (mette in relazione $w$, che è la pendenza della retta con i moltiplicatori di Lagrange).<br>\n",
    "Dice che $w$, il vettore che serve per calcolare la pendenza della retta, viene calcolato ignorando tutti i punti distanti, dal momento che per tutti gli $x_i$ che hanno un moltiplicatore $\\alpha$ uguale a 0 non vengono considerati. I punti che servono per calcolare la retta sono quelli più vicini che hanno $\\alpha \\gt 0$ &rarr; ***support vectors***.\n",
    "\n",
    "Se derivo rispetto a $b$ ottengo:\n",
    "\n",
    "### $\\sum_{i=1}^N \\alpha_i y_i = 0$\n",
    "\n",
    "Se applico entrambe le derivate alla funzione Lagrangiana ottengo:\n",
    "\n",
    "### $L(w,b,a) = \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (x_i^T x_j$)\n",
    "\n",
    "E adesso questa funzione è solo in funzione di $\\alpha$, e quindi la posso risolvere.\n",
    "\n",
    "Questo equazione si chiama forma duale di WOLFE, che rispetto a prima va massimizzata (in quanto problema duale):\n",
    "\n",
    "### $max_{\\alpha} \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (x_i^T x_j)$\n",
    "\n",
    "### soggetta ai vincoli $\\alpha_i \\geq 0$ e $\\sum_{i=1}^N \\alpha_i y_i = 0$\n",
    "\n",
    "La soluzione contiene due tipologie di moltiplicatori $\\alpha_i$:\n",
    "- punti che hanno $\\alpha_i = 0$ &rarr; $x_i$ è irrilevante\n",
    "- punti che hanno $\\alpha_i \\gt 0$ &rarr; $x_i$ è un support vector\n",
    "\n",
    "Alla fine, una volta che ho trovato $w$ come la uso? Basta fare:\n",
    "\n",
    "### $f(x) = w^T x$\n",
    "\n",
    "E so che se sono > 0 sono della classe 1 e se sono < 0 sono della classe -1.\n",
    "\n",
    "Se sono nel duale, al posto di $w$ posso metter la formula $\\sum_{i=1}^N \\alpha_i y_i x_i$.\n",
    "\n",
    "### $f(x) = \\sum_{i=1}^N \\alpha_i y_i x_i^T x$\n",
    "\n",
    "Dove: \n",
    "- $x$ è il nuovo punto che devo classificare\n",
    "- $\\alpha_i y_i x_i^T$ viene dal dataset di training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non separable soft margin"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
