{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La ***<span style=\"color:gold;\">classificazione binaria</span>*** può essere vista come il compito di separare le classi in uno spazio di feature.\n",
    "\n",
    "![SVM1](./images/SVM1.png)\n",
    "\n",
    "\n",
    "Posso scegliere molte rette per dividere questi punti:\n",
    "\n",
    "![SVM2](./images/SVM2.png)\n",
    "\n",
    "Mi piacerebbe scegliere quella retta che non passa esattamente vicino a uno di questi punti, bensì una che sia il più lontana possibile dai punti, perché mi permette di costruire una \"banda di sicurezza\" tra le due classi\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Margin\n",
    "Il <span style=\"color:gold;\">margine</span> è lo spazio intorno alla retta di separazione in cui non cadono punti. Per semplicità il margine viene sempre costruito simmetrico\n",
    "\n",
    "Per costruirlo utilizzo l'equazione della distanza punto-retta:\n",
    "\n",
    "### $r_i = \\frac{y_i(w^Tx_i + b)}{||w||}$\n",
    "\n",
    "dove $x_i$ è il punto in cui devo calcolare la distanza.\n",
    "\n",
    "Quindi io voglio trovare la retta in cui se io faccio la distanza punto-retta tra i punti più vicini alla retta, essa è massima.\n",
    "\n",
    "![SVM3](./images/SVM3.png)\n",
    "\n",
    "Non tutti i punti sono uguali e concorrono allo stesso modo a scegliere la retta migliore, ma solamente quelli più vicini alla retta. <br>\n",
    "Essi si chiamano anche <span style=\"color:gold;\">vettori di supporto</span>.\n",
    "\n",
    "---\n",
    "\n",
    "Dato un dataset $D = (x_i, y_i)_{i\\in[1..N]}$, con la label $y_i \\in {+1, -1}$ e con $\\rho$ come margine, vogliamo:\n",
    "\n",
    "- #### se $y_i = +1$ allora $w^Tx_i + b \\geq \\rho$, ovvero che tutti i punti che stanno sopra la retta ci stiano almeno per $\\frac{\\rho}{2}$\n",
    "- #### se $y_i = -1$ allora $w^Tx_i + b \\leq -\\rho$, ovvero che tutti i punti che stanno sotto alla retta ci stiano almeno per $\\frac{\\rho}{2}$\n",
    "\n",
    "Posso scrivere queste due disequazioni in un'equazione unica (giocando sul fatto che le label sono $+1$ e $-1$):\n",
    "\n",
    "### $y_i(w^Tx_i + b) \\geq \\rho,   \\forall x_i \\in D$\n",
    "\n",
    "Per tutti i vettori di supporto (quelli cerchiati), questa disuguaglianza in realtà è un'uguaglianza:\n",
    "\n",
    "### $w^Tx_i + b = \\frac {\\rho}{2}$\n",
    "\n",
    "Che riscrivo come:\n",
    "\n",
    "### $r_s = \\frac{y_i(w^Tx_i + b)}{||w||} = \\frac{\\rho}{2||w||}$\n",
    "\n",
    "Prendo il vettore $w$ che non conosco, e divido la quantità per $\\frac {\\rho}{2}$, e lo chiamo $\\hat w$ ottenendo:\n",
    "\n",
    "###  $\\frac{\\rho}{2||w||} = \\frac {1}{||\\hat w||}$\n",
    "\n",
    "Quindi il mio margine è 2 volte $r_s$, ovvero (da questo momento verranno usati $w$ e $\\hat w$ in modo intercambiabile dato che è il set di parametri che voglio trovare):\n",
    "\n",
    "### $\\rho = 2r_s = \\frac{2}{||w||} = \\frac{2}{||\\hat w||}$\n",
    "\n",
    "Il mio classificatore dovrà quindi risolvere una disequazione tale per cui dovrà massimizzare $\\frac{2}{||w||}$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM Quadratic Problem\n",
    "\n",
    "Formula di massimizzazione:\n",
    "\n",
    "### $argmax_{w,b} \\frac{2}{||w||}$\n",
    "\n",
    "Con i vincoli:\n",
    "\n",
    "### $y_i(w^Tx_i + b) \\geq 1,  \\forall (x_i, y_i) \\in D$\n",
    "\n",
    "dove posso scrivere $\\geq 1$ perché ho diviso tutto per $\\rho/2$:\n",
    "\n",
    "![SVM4](./images/SVM4.png)\n",
    "\n",
    "Quindi voglio trovare $w$ e $b$ tali per cui il margine è massimo, ma che allo stesso tempo soddisfino i vincoli.\n",
    "\n",
    "Dato che $\\frac{2}{||w||}$ è massimo quando $||w||$ è minimo, posso riscrivere il problema come:\n",
    "\n",
    "### $argmin_{w,b} ||w||^2$\n",
    "\n",
    "Con i vincoli:\n",
    "\n",
    "### $y_i(w^Tx_i + b) \\geq 1,  \\forall (x_i, y_i) \\in D$\n",
    "\n",
    "Siccome la radice nella norma è problematica, elevo tutto al quadrato.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# QP Solvers\n",
    "\n",
    "In generale, i problemi di minimizzazione quadratica soggetti a vincoli lineari (<span style=\"color:gold;\">QP</span>) vengono risolti da software &rarr; <span style=\"color:gold;\">QP solver</span>. \n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Optimization (ottimizzazione vincolata)\n",
    "\n",
    "Normalmente, un problema di ottimizzazione con vincoli di disuguaglianza si imposta come:\n",
    "\n",
    "### $min f(x)$ <br>\n",
    "### soggetta ai vincoli $h_k(x) \\geq 0, for k \\in 1, ..., K$\n",
    "\n",
    "Devo incorporare i miei vincoli nella equazione da risolvere utilizzando i moltiplicatori di Lagrange, ottenendo una funzione Lagrangiana in cui ho ancora la mia funzione che voglio risolvere $f(x)$ - la sommatoria di un moltiplicatore, uno per ogni vincolo, moltiplicato per il vincolo stesso:\n",
    "\n",
    "### $L(x, \\mu) = f(x) - \\sum_{k=1}^K \\alpha_i h_k(x), \\forall \\alpha_i \\geq 0$\n",
    "\n",
    "dove $\\alpha_i$ sono i moltiplicatori di Lagrange, ovvero un numero o positivo o nullo che moltiplica il vincolo.\n",
    "\n",
    "Ci possono essere due casi:\n",
    "- In tutti i casi in cui \"qua dentro ci si mette\" $x_i$ e il vincolo è soddisfatto, il moltiplicatore di Lagrange varrà 0 (quel vincolo è già soddisfatto e non devo considerarlo), il vincolo viene considerato inattivo e la soluzione del problema sarebbe la $x$ che annulla la derivata.\n",
    "- Se però il vincolo non è soddisfatto e vale l'uguaglianza, cioè $x_i$ annulla il mio vincolo ($h_i(x_i) = 0$) allora il moltiplicatore di Lagrande deve essere attivo ($\\alpha > 0$) e quindi la soluzione sarà quella che annulla le derivate - il vettore dei moltiplicatori di Lagrange per le derivate dei vincoli:\n",
    "\n",
    "![SVM5](./images/SVM5.png)<br>Quindi deve annullare sia la derivata della funzione che la derivata del vincolo.\n",
    "\n",
    "Il triangolo verso il basso indica un prodotto riga per colonna(?).\n",
    "\n",
    "Spiegazione:\n",
    "\n",
    "![SVM6](./images/SVM6.png)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constrained Optimization\n",
    "\n",
    "Mettendo tutto insieme si ottiene otteniamo le <span style=\"color:gold;\">KKT</span>, ovvero le condizioni necessarie affinché il nostro problema vincolato sia risolvibile.\n",
    "\n",
    "La prima condizione indica che la soluzione annulli questa equazione (di fatto la derivata):<br>\n",
    "### $\\nabla f(x*) - \\alpha^T \\nabla h(x*) = 0$\n",
    "\n",
    "La seconda condizione era vera fin dall'inizio, cioè dato che $\\alpha_i$ sono moltiplicatori di Lagrange possono essere solo positivi o nulli:<br>\n",
    "### $\\alpha_i \\geq 0$\n",
    "\n",
    "La terza condizione ci dice che se uno dei vincoli non è nullo il suo moltiplicatore dev'essere 0, e viceversa se h è nullo, il suo moltiplicatore dev'essere positivo:<br>\n",
    "### $\\alpha^T h(x) = 0$\n",
    "\n",
    "La quarta condizione dice che la mia soluzione deve comunque rispettare i vincoli (questo succederà per costruzione)\n",
    "### $h(x*) \\leq 0$\n",
    "\n",
    "\n",
    "Se il vincolo è una funzione affine, e sostanzialmente lineare, $f$ è convessa e $g$ è differenziabile in \"modo infinito\", le ***KKT*** sono sia condizioni necessarie che sufficienti, quindi se sono verificate sono sicuro che ci sia una soluzione.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear SVM Separable Case\n",
    "\n",
    "Quindi, nel caso dell'SVM il mio problema è questo:\n",
    "\n",
    "### $argmin_{w,b} ||w||^2$\n",
    "\n",
    "Dove voglio minimizzare $||w||^2$ soggetta a questi vincoli:\n",
    "\n",
    "### $y_i(w^Tx_i + b) \\geq 1,  \\forall (x_i, y_i) \\in D$\n",
    "\n",
    "Con $D$ composto da $N$ elementi, la funzione Lagrangiana diventa:\n",
    "\n",
    "### $min_{w,b,\\alpha} L(w,b,\\alpha) = \\frac{1}{2}||w||^2 - \\sum_{i=1}^N \\alpha_i(y_i(w^Tx_i + b) - 1)$\n",
    "\n",
    "Che ha come KKT $\\forall i = 1 .. N$:\n",
    "\n",
    "### $\\alpha_i \\geq 0$\n",
    "\n",
    "### $y_i(w^Tx_i+b)-1 \\geq 0$\n",
    "\n",
    "### $\\alpha_i(y_i(w^Tx_i+b)-1) = 0$\n",
    "\n",
    "Per trovare il minimo posso derivare e porre uguale a 0, e dato che dipende da 3 variabili dovrò derivare 3 volte.\n",
    "\n",
    "Inizio con la derivata rispetto a $w$ ottengo:\n",
    "\n",
    "### $w = \\sum_{i=1}^N \\alpha_i y_i x_i$\n",
    "\n",
    "Che è l'equazione più importante perché mette in relazione $w$ al moltiplicatore, in quanto posso riscriverlo in funzione di $\\alpha$ &rarr; relazione tra primale e duale (mette in relazione $w$, che è la pendenza della retta con i moltiplicatori di Lagrange).<br>\n",
    "Dice che $w$, il vettore che serve per calcolare la pendenza della retta, viene calcolato ignorando tutti i punti distanti, dal momento che per tutti gli $x_i$ che hanno un moltiplicatore $\\alpha$ uguale a 0 non vengono considerati. I punti che servono per calcolare la retta sono quelli più vicini che hanno $\\alpha \\gt 0$ &rarr; ***support vectors***.\n",
    "\n",
    "Se derivo rispetto a $b$ ottengo:\n",
    "\n",
    "### $\\sum_{i=1}^N \\alpha_i y_i = 0$\n",
    "\n",
    "Se applico entrambe le derivate alla funzione Lagrangiana ottengo:\n",
    "\n",
    "### $L(w,b,a) = \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (x_i^T x_j$)\n",
    "\n",
    "E adesso questa funzione è solo in funzione di $\\alpha$, e quindi la posso risolvere.\n",
    "\n",
    "Questo equazione si chiama forma duale di WOLFE, che rispetto a prima va massimizzata (in quanto problema duale):\n",
    "\n",
    "### $max_{\\alpha} \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (x_i^T x_j)$\n",
    "\n",
    "### soggetta ai vincoli $\\alpha_i \\geq 0$ e $\\sum_{i=1}^N \\alpha_i y_i = 0$\n",
    "\n",
    "La soluzione contiene due tipologie di moltiplicatori $\\alpha_i$:\n",
    "- punti che hanno $\\alpha_i = 0$ &rarr; $x_i$ è irrilevante\n",
    "- punti che hanno $\\alpha_i \\gt 0$ &rarr; $x_i$ è un support vector\n",
    "\n",
    "Alla fine, una volta che ho trovato $w$ come la uso? Basta fare:\n",
    "\n",
    "### $f(x) = w^T x$\n",
    "\n",
    "E so che se sono > 0 sono della classe 1 e se sono < 0 sono della classe -1.\n",
    "\n",
    "Se sono nel duale, al posto di $w$ posso metter la formula $\\sum_{i=1}^N \\alpha_i y_i x_i$.\n",
    "\n",
    "### $f(x) = \\sum_{i=1}^N \\alpha_i y_i x_i^T x$\n",
    "\n",
    "Dove: \n",
    "- $x$ è il nuovo punto che devo classificare\n",
    "- $\\alpha_i y_i x_i^T$ viene dal dataset di training\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non separable soft margin\n",
    "\n",
    "Se per qualche motivo il dataset non è separabile perfettamente con una retta, ci deve essere qualche vincolo che devo violare \n",
    "\n",
    "![SVM7](./images/SVM7.png)\n",
    "\n",
    "Quindi esiste una versione dell'SVM per dataset non separabile che utilizza delle variabili \"artificiali\" per gestire il caso in cui ci sono dei vincoli che posso violare. <br>Il problema viene costruito in questo modo:\n",
    "\n",
    "Ad ogni elemento associo anche una varaibile di ***slack*** in più $\\xi_i \\geq 0$ che mi dice quanto sto violando il vincolo. Nello specifico è 0 se l'elemento non viola il vincolo ed è maggiore di 0 se lo viola e mi permette di ***traslare*** l'elemento in una posizione in cui non viola il vincolo:\n",
    "\n",
    "### $y_i(w^Tx_i + b) \\geq 1 - \\xi_i,  \\forall (x_i, y_i) \\in D$\n",
    "\n",
    "Il problema di ottimizzazione diventa:\n",
    "\n",
    "### $min \\frac{1}{2}||w||^2 + C \\sum_{i=1}^N \\xi_i$\n",
    "\n",
    "#### soggetto a $y_i(x_i w + b) \\geq 1 - \\xi_i$, che cambia a sua volta aggiungendo a destra la variabile di slack in modo che tutti i vincoli siano soddisfatti\n",
    "\n",
    "Tutto cercando di violare meno vincoli possibile con gli $\\xi_i$ più piccoli.<br>\n",
    "Tra queste funzioni che voglio minimizzare c'è un iperparametro $C$ che regola **quanto è importante violare i vincoli rispetto a minimizzare il margine**\n",
    "\n",
    "Tecnicamente con un **C** basso si riesce a traslare di più mentre con un **C** basso riesco a generalizzare meglio.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Non Linear SVM\n",
    "\n",
    "Se ho dei dati che non sono separabili da una linea mi potrei trovare in una situazione simile:\n",
    "\n",
    "![SVM8](./images/SVM8.png)\n",
    "\n",
    "L'SVM è l'unico classificatore che nasce come lineare e ha la possibilità di diventare non lineare. \n",
    "\n",
    "Posso dunque spostarmi in un altro spazio dimensionale, con un numero di dimensioni maggiore in cui posso poi separare i dati in modo lineare. Quindi devo trovare una qualche funzione $\\phi$ che mi fa passare dal mio vettore delle feature originali $x$ &rarr; $\\phi(x)$\n",
    "\n",
    "![SVM9](./images/SVM9.png)\n",
    "\n",
    "Ad aumentare del numero delle dimensioni, il problema tende a linearizzarsi.\n",
    "\n",
    "L'SVM permette di definire il mapping $\\phi$ : $x$ &rarr; $\\phi(x)$ in modo implicito dove al posto di $x$ uso $\\phi(x)$.\n",
    "\n",
    "Richiamando la formula dell'SVM duale:\n",
    "\n",
    "### $L(\\alpha) = \\sum_{i=1}^N \\alpha_i - \\frac{1}{2} \\sum_{i=1}^N \\sum_{j=1}^N \\alpha_i \\alpha_j y_i y_j (x_i^T x_j)$\n",
    "\n",
    "### $f(x) = \\sum_{i=1}^N \\alpha_i y_i (x_i^T x)$\n",
    "\n",
    "La forma duale rimane uguale se non che avrò:\n",
    "\n",
    "### $\\phi(x_i)^T \\phi(x_j) = K(x_i, x_j)$\n",
    "\n",
    "Che è ancora un prodotto vettoriale riga colonna e quindi uno scalare, con la differenza che non sono più nello spazio originale ma in quello trasformato. Se definisco $K(x_i, x_j)$, come il risultato del prodotto vettoriale nel mio spazio trasformato, posso riscrivere come:\n",
    "\n",
    "### $f(x) = \\sum_{i=1}^N \\alpha_i y_i K(x_i, x)$\n",
    "\n",
    "$K$ è quello che si chiama <span style=\"color:gold;\">funzione kernel</span>, che è una funzione che mi permette di calcolare il prodotto vettoriale tra due vettori in uno spazio trasformato senza dover fare la trasformazione esplicita.\n",
    "\n",
    "\"È un po' un paradosso perché mi sto spostando in un altro spazio vettoriale (che non so quale sia) e sto affermando che il prodotto vettoriale faccia un numero\".\n",
    "\n",
    "La cosa importante è che è sufficiente che la ***funzione kernel*** soddisfi qualche proprietà, affinché diventi una funzione ammissibile.\n",
    "\n",
    "Classificare con una funzione di kernel è molto più inefficiente che classificare con il primale, perché devo calcolare molte distanze. Per classificare il mio $x$ devo calcolare le distanze tra $x$ e tutti i support vector. Quindi siccome non ho le coordinate di un punto nel mio spazio trasformato, lo devo identificare misurando la sua distanza tra tutti i support vector.\n",
    "\n",
    "***Nota***: Quando uso i kernel la SVM è sempre nel duale e non posso più passare al primale perché la versione originale lineare lavora nello spazio trasformato, di cui non so la funzione di mapping.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernel Trick\n",
    "\n",
    "Bisogna quindi capire quali funzioni possono essere considerate dei miei kernel.\n",
    "\n",
    "Se io considero tutti i punti del mio dataset e costruisco la ***matrice kernel***:\n",
    "\n",
    "![SVM10](./images/SVM10.png)\n",
    "\n",
    "Se questa matrice è:\n",
    "- simmetrica, quindi $K(x_1, x_2)$ sia uguale a $K(x_2, x_1)$\n",
    "- semi-definita positiva, cioè il suo determinante è $\\geq 0$ (per averla semi-definita positiva è sufficiente che la funzione stia tra 0 e i numeri reali positivi, ovvero non sia mai negativa)\n",
    "\n",
    "Allora si chiama <spam style=\"color:gold;\">matrice di Gram</spam> e la mia funzione che ho scelto è un kernel.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Popular Kernels\n",
    "\n",
    "Ci sono dei kernel famosi che sono consigliati:\n",
    "- ##### ***Lineare***: il più semplice è nessun kernel, cioè $K(x, x_i) = x^T x_i$\n",
    "- ##### ***Polinomiale***: $K(x, x_i) = (1 + x^T x_i)^p$ e mappa in $\\binom{d + p}{p}$ dimensioni (che è il numero di termini di un polinomio)\n",
    "- ##### ***RBF*** (o Kernel Gaussiano): $K(x, x_i) = e^{-\\frac{||x-x_i||^2}{2\\sigma^2}}$, \n",
    "    Dove $\\sigma$ è un parametro che gli fornisco io e si chiama **banda del kernel**. A seconda di come metto $\\sigma$, questo kernel può essere più morbido o più appuntito nel contesto di una gaussiana.<br>\n",
    "    Se si mette una banda molto stretta si può essere molto sensibili alle differenze, oppure invarianti alle piccole differenze e quindi più sensibili alle grandi differenze.<br>\n",
    "    Questo è un mapping implicito in uno spazio che ha un numero di dimensioni pari a infinito. In uno spazio infinito-dimensionale, il mio numero di dimensioni è talmente alto che sarà sicuramente una funzione lineare che separa i dati.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimostrazione RBF\n",
    "\n",
    "Il kernel gaussiano è la versione normalizzata di: \n",
    "\n",
    "## $e^{-\\frac{x^T x_i}{\\sigma^2}}$\n",
    "\n",
    "Cioè, il prodotto scalare diviso la norma degli elementi a nominatore.\n",
    "\n",
    "Dato che lo sviluppo in serie di $e^x$ è:\n",
    "\n",
    "## $e^x = \\sum_{n=1}^{\\infty} \\frac{x^n}{n!}$\n",
    "\n",
    "E quindi lo sviluppo in serie di $e^{\\frac{x^T x_i}{\\sigma^2}}$ è:\n",
    "\n",
    "## $e^{\\frac{x^T x_i}{\\sigma^2}} = \\sum_{n=1}^{\\infty} \\frac{(x^T x_i)^n}{\\sigma^{2n}n!}$\n",
    "\n",
    "A questo punto considero ogni termine dello sviluppo in serie una feature del mio nuovo feature vector. Quindi questa sommatoria a infinito avrà infinite feature.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing Kernels\n",
    "\n",
    "Quando ho classificato, posso tornare nello spazio originale e vedere come diventa la classificazione, otterrò delle superfici di separazione non lineari nello spazio originale, ma lineari nello spazio trasformato:\n",
    "\n",
    "![SVM11](./images/SVM11.png)\n",
    "\n",
    "# Contro:\n",
    "\n",
    "- complessità computazionale nello spazio trasformato: quando classifico con il duale, se ho tanti support vector, quando devo trovare un nuovo punto $x$ è molto lenta.\n",
    "- più faccio la mia superficie di separazione complicata, più sono sensibile al caso che mi sta presentando il mio training set, quindi generalizza di meno &rarr; ***overfitting***.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
