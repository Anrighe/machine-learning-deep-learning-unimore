{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Consente di raggruppare elementi simili in gruppi chiamati cluster. Per poter raggrupparli è necessario che ci sia un'alta similiarità tra gli elementi della stessa classe e una basse similiarità tra gli elementi di classi diverse.\n",
    "\n",
    "In fase di design è necessario scegliere le feature su cui fare clustering e la funzione di similiarità.\n",
    "\n",
    "La similiarità deve soddifare le seguenti proprietà:\n",
    "- Simmetria D(A, B) = D(B, A)\n",
    "- Costanza della similiarità D(A, A) = 0\n",
    "- Positività D(A, B) = 0 se e solo se A = B\n",
    "- Disuguaglianza triangolare D(A, B) ≤ D(A, C) + D(B, C)\n",
    "\n",
    "Utilizzando il piano euclideo, la distanza euclidea è a tutti gli effetti una misura della similiarità.\n",
    "\n",
    "There are two types of Clustering\n",
    "Partitional algorithms: Construct various partitions and then evaluate them by\n",
    "some criterion\n",
    "Hierarchical algorithms: Create a hierarchical decomposition of the set of objects\n",
    "using some criterion\n",
    "\n",
    "\n",
    "Per fare clustering ci sono due strategie:\n",
    "- Partizionale: tipo di clustering iterativo in cui si costruiscono varie partizioni e poi si valutano con un criterio in modo gerarchico. Viene costruito un albero di merge o split.\n",
    "- Gerarchico: in un unico step divide già tutti gli elementi nei gruppi, creandoa una decomposizione gerarchica dell'insieme di oggetti usando un criterio.\n",
    "\n",
    "Qualunque sia la modalità di clustering le proprietà richieste a un algoritmo di clustering sono:\n",
    "- Scalabilità\n",
    "- Capacità di gestire di dati diversi\n",
    "- Cercare di avere minima conoscenza del dominio possibile (come prerequisito dovrei sapere il meno possibile dei dati che sto trattando)\n",
    "- Deve essere in grado di gestire il rumore e gli outlier, cioè i dati che non appartengono a nessun cluster\n",
    "- Non devono dipendere dall'ordine in cui si presentano i dati\n",
    "- Devono poter incorporare dei vincoli\n",
    "- Deve essere interpretabile\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "Dal momento che non possiamo testare tutti gli alberi possibili dobbiamo fare una ricerca euristica di tutti gli alberi possibili. Possiamo farlo in due modi:\n",
    "- ***Bottom-Up*** (agglomerativo): partendo da ogni elemento in un proprio cluster, trova la migliore coppia da unire in un nuovo cluster. Ripeti fino a quando tutti i cluster sono uniti. \n",
    "- ***Top-Down*** (divisivo): partendo da tutti i dati in un unico cluster, considera ogni possibile modo di dividere il cluster in due. Scegli la migliore divisione e operare ricorsivamente su entrambi i lati.\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering1](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering1.png)\n",
    "\n",
    "Se due elementi sono connessi alla stessa riga, allora appartengono allo stesso gruppo.\n",
    "\n",
    "Una volta costrutito l'albero o ***dendogramma***, per ottenere i cluster bisogna tagliare l'albero in un certo punto. Per fare ciò bisogna scegliere un valore di soglia.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start with a Distance or Similarity\n",
    "\n",
    "Per aggregare gli elementi in fase iniziale, si costruisce una matrice in cui nelle righe e nelle colonne ci sono gli elementi, mentre nella casella in corrispondenza di due elementi c'è la distanza o la similarità tra quegli elementi.\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering2](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering2.png)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottom-Up (agglomerativo)\n",
    "\n",
    "All'inizio parto con tutti gli elementi disgiunti e con una misura di distanza/similiarità tra gli elementi.\n",
    "\n",
    "Per tutte le possibili coppie dei miei elementi misuro la loro distanza e unisco i due elementi che hanno la distanza minore.\n",
    "\n",
    "### Tecniche di Linkage: strategie per il calcolo della distanza tra due cluster\n",
    "\n",
    "- **Single Linkage** (nearest neighbor): la distanza tra due cluster è determinata dalla distanza tra i due oggetti più vicini nei due cluster diversi (crea cluster molto elungati)\n",
    "- **Complete Linkage** (furthest neighbor): la distanza tra due cluster è determinata dalla distanza tra i due oggetti più lontani nei due cluster diversi (crea cluster molto compatti)\n",
    "- **Group Average Linkage**: la distanza tra due cluster è determinata dalla media delle distanze tra tutti gli oggetti nei due cluster diversi\n",
    "- **Wards Linkage**: (è la media diviso la varianza per la distanza e media per la varianza per la similiarità) &rarr; si cerca di minimizzare la varianza dei cluster uniti\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering3](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering3.png)\n",
    "\n",
    "Smetto di aggregare quando la distanza elemento-gruppo o gruppo-gruppo è sopra una certa soglia.\n",
    "\n",
    "Quindi il clustering gerarchico è accompagnato da due iperparametri:\n",
    "- la strategia di aggregazione\n",
    "- la soglia oltre la quale devo smettere di aggregare\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Riassumendo: hierarchical clustering\n",
    "\n",
    "- Non devo stabilire a priori il numero di cluster, però devo definire la soglia\n",
    "- La natura gerarchica è intuitiva per l'uomo &rarr; il dendogramma è facile da interpretare\n",
    "- Non scala bene: perché ogni volta devo calcolare la distanze con tutti gli altri elementi &rarr; $O(n^2)$, dove $n$ è il numero totale di elementi\n",
    "- Come tutti gli algoritmi di ricerca euristica, i locali ottimi sono un problema &rarr; non è detto che un determinato partizionamento sia il migliore a livello globale\n",
    "- L'interpretazione dei risultati è soggettiva\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial clustering\n",
    "\n",
    "Nongerarchica, ogni istanza è inserita in uno dei K cluster non sovrapposti.\n",
    "\n",
    "Dal momento che viene restituito un solo insieme di cluster, l'utente deve normalmente inserire il numero desiderato di cluster K.\n",
    "\n",
    "Quindi è necessario avere il numero di cluster desiderati.\n",
    "\n",
    "# K-means\n",
    "\n",
    "Algoritmo di clustering che utilizza la funzione <span style=\"color:gold;\">Squared Error</span>\n",
    "\n",
    "Dato un set di punti ($x_1, x_2, ..., x_n$) e $k$ insiemi $S = \\{S_1, S_2, S_3, ..., S_k\\}$.\n",
    "\n",
    "La funzione obiettivo è:\n",
    "\n",
    "### $argmin_S \\sum_{i=1}^k \\sum_{x \\in S_i} ||x - \\mu_i||^2$\n",
    "\n",
    "dove $\\mu_i$ è la media dei punti in $S_i$.\n",
    "\n",
    "&rarr; ***Quindi cerco di minimizzare la distanza tra i punti e il centro del cluster.***\n",
    "\n",
    "La funzione obiettivo sopra è uguale a:\n",
    "\n",
    "### $argmin_S \\sum_{i=1}^k \\frac{1}{|S_i|} Var(S_i)$\n",
    "\n",
    "Questo è equivalente a:\n",
    "\n",
    "### $argmin_S \\sum_{i=1}^k \\sum_{x,y \\in S_i} ||x - y||^2$\n",
    "\n",
    "# Proof\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering4](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering4.png)\n",
    "\n",
    "Quindi voglio trovare le distanze minime per cui sommando tutte le distanze dal centro dei cluster rossi in figura ottengo la distanza minima.\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering5](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering5.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo K-means Naive procedure\n",
    "\n",
    "\n",
    "Inizializza $k$ cluster centers randomici\n",
    "\n",
    "***while*** i centri non sono aggiornati o il numero massimo di iterazioni è stato raggiunto:\n",
    "\n",
    "- Assegna ogni punto al centro più vicino\n",
    "- Calcola per ogni cluster la media dei punti assegnati\n",
    "- $\\mu^{new}_{S_i} = \\Sigma{x \\in S_i} \\frac{x}{|S_i|}$\n",
    "- $\\mu_{S_i} = \\mu^{new}_{S_i}$\n",
    "\n",
    "***end***\n",
    "\n",
    "*A ogni update, aggiorno la posizione del centro cluster e il nuovo centro diventa la media delle coordinate (feature) dei punti che sono stati assegnati a quel cluster.*\n",
    "\n",
    "#### Iterazione 1: fase di assegnamento dei punti\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering6](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering6.png)\n",
    "\n",
    "#### Iterazione 2: assengno i punti al centro più vicino\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering7](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering7.png)\n",
    "\n",
    "#### Iterazione 3: il centro si sposta perché è uguale alla media di tutte le coordinate dei punti assegnati al cluster\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering8](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering8.png)\n",
    "\n",
    "#### Iterazione 4: nuova fase di asegnamento + update\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering9](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering9.png)\n",
    "\n",
    "Non è detto che scegliere i centi iniziali a caso portino sempre alla stessa soluzione! È bene sceglierli casualmente ma cercando di ricoprire tutto lo spazio (range min-max di ogni feature).\n",
    "\n",
    "Un'altra soluzione potrebbe essere lanciare tante volte l'algoritmo e scegliere la soluzione migliore, ovvero quelle che coinvolgono partizionamenti che capitano più spesso.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments on kmeans\n",
    "\n",
    "Pro:\n",
    "- Relativamente efficiente: $O(tkn)$, dove $n$ è il numero di oggetti, $k$ è il numero di cluster e $t$ è il numero di iterazioni. Normalmente $k$ e $t$ sono molto più piccoli di $n$.\n",
    "- Spesso termina in un ottimo locale. L'ottimo globale può essere trovato utilizzando tecniche come il *deterministic annealing* e gli *algoritmi genetici*\n",
    "\n",
    "Contro:\n",
    "- **<u>Applicabile solo quando la media è definita, quindi cosa succede con i dati categorici?</u>** &rarr; non posso usare la media per i quei dati che possono essere solo booleani, ad esempio.\n",
    "- Bisogna specificare $k$, il numero di cluster, in anticipo\n",
    "- Non è in grado di gestire dati rumorosi e outlier\n",
    "- Non è adatto per scoprire cluster con forme non convesse\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning Around Medoids\n",
    "\n",
    "Per ovviare al problema del K-Means quando non posso fare la media.\n",
    "\n",
    "Se al posto di creare dei nuovi punti (la media) utilizzo dei punti che sono già all'interno del mio dataset, sono sicuro di rimanere nel \"dominio\" di questo problema.\n",
    "\n",
    "Trova gli oggetti rappresentativi, chiamati medoids, nei cluster.\n",
    "- PAM (Partitioning Around Medoids, 1987)\n",
    "- Parte da un set iniziale di medoidi e iterativamente sostituisce uno dei medoids con uno dei non-medoidi se migliora la distanza totale del clustering risultante\n",
    "- PAM funziona efficacemente per piccoli set di dati, ma non scala bene per grandi set di dati\n",
    "\n",
    "Algoritmo:\n",
    "1. Seleziona $k$ punti a caso del mio dataset e ogni elemento è assegnato al centro più vicino\n",
    "2. Il centro di un cluster è il punto che ha distanza minima da tutti gli altri punti del cluster &rarr; questo è il nuovo centro\n",
    "3. Reitero a convergenza come nel K-means\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ho delle superfici di separazioni che sono degli sferoidi, il K-means non funziona bene perché non è in grado di gestire superfici non convesse.\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering10](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering10.png)\n",
    "\n",
    "Per ovviare a questo problema si possono usare dei grafi, in particolare i grafi di similiarità, che mappano i punti in un grafo in cui i nodi sono i punti e gli archi sono le distanze tra i punti (similarità).\n",
    "\n",
    "***Grafo bipartito***: se c'è la possibilità di dividere il grafo e dividerlo in due gruppi disgiunti.\n",
    "\n",
    "Vettore dei pesi dei vertici: $w = (w_1, w_2, ..., w_n)$, dove $w_i$ è il peso del vertice $i$.\n",
    "\n",
    "Matrice di adiacenza: $A = (a_{ij})$, dove $a_{ij}$ è il peso dell'arco tra il vertice $i$ e il vertice $j$.\n",
    "\n",
    "Campo dei vertici: operatore che va dai vertici ai numeri reali $L2(V) = {f:V \\rightarrow R}$. Si tratta di un vettore che ha tante componenti quanti sono i vertici che associa a ogni vertice un numero reale $f=(f_1, f_2, ..., f_n)$. (Ad esempio potrebbe avere 1 e 0 in corrispondenza ai vertici che voglio selezionare, ecc..)\n",
    "\n",
    "Il campo permette di definire il prodotto scalare tra due grafi (<span style=\"color:gold;\">Spazio di Hilbert</span>):\n",
    "\n",
    "#### $<f, g>_{L2(V)} = \\sum_{i \\in V} a_i f_i g_i$\n",
    "\n",
    "Ovvero il prodotto tra $f$ e $g$, ovvero due campi diversi è la somma di $i$ che scorre su tutti i vertici moltiplicando il peso del vertice $a_i$ con il prodotto dei due campi $f_i$ e $g_i$.\n",
    "\n",
    "Lo ***spazio di Hilbert*** esiste qualora esista il prodotto scalare tra due campi. Lo spazio euclideo è anche uno spazio di Hilbert, ma uno spazio di Hilbert non è necessariamente euclideo, perché non è detto che sia equipaggiato con il concetto di distanza (magari non vale la disuguaglianza triangolare).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graph Calculus\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering11](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering11.png)\n",
    "\n",
    "- <span style=\"color:gold;\">Gradiente</span>: $L2(V) \\rightarrow L2(E)$ (funzione definita dai vertici agli archi)<br>\n",
    "    ###  $( \\triangledown f)_{ij} = \\sqrt {w_{ij}} (f_i - f_j)$<br>\n",
    "    Il gradiente di $ij$ (due vertici) è il valore della radice di $w_{ij}$ per il valore del campo del vertice $i$ meno il valore del campo del vertice $j$.\n",
    "\n",
    "- <span style=\"color:gold;\">Divergenza</span>: $L2(E) \\rightarrow L2(V)$ (funzione definita dagli archi ai vertici)<br>\n",
    "    ### $div(F)_i = \\frac{1}{\\alpha_i} \\sum_{(i,j) \\in E} \\sqrt {w_{ij}} (F_{ij} - F_{ji})$<br>\n",
    "    La divergenza di $i$ è la somma dei vertici vicini a $i$ della radice di $w_{ij}$ per la differenza tra il valore del campo dell'arco $ij$ e il valore del campo dell'arco $ji$.\n",
    "\n",
    "- <span style=\"color:gold;\">Operatore laplaciano</span>: $L2(V) \\rightarrow L2(V)$ (funzione definita dai vertici ai vertici)<br>\n",
    "    ### $(\\Delta f)_i = \\frac {1}{\\alpha_i} \\sum_{j:(i,j) \\in E} \\sqrt {w_{ij}} (f_i - f_j)$<br>\n",
    "    L'operatore laplaciano è $1$ su $\\alpha_i$ dove $\\alpha_i$ è il peso del vertice per la somma per tutti i $j$ collegati al vertice $i$ (quindi blocco un vertice e per tutti gli archi che entrano) di $w_{ij}$ per la differenza tra il valore del campo del vertice $i$ e il valore del campo del vertice $j$.<br>\n",
    "    Il laplaciano è l'analogo dei grafi della derivata seconda ed è rappresentato in notazione matriciale come una matrice $n*n$ in cui $n$ è il numero di vertici (simmetrica e definita positiva), e si calcola in questo modo:<br>\n",
    "    ### $\\Delta = A^{-1}(D - W)$<br>\n",
    "    Dove $A = (\\frac{1}{\\alpha_i})$  è la matrice dei valori dei coefficienti dei vertici (matrice diagonale in cui in ogni casella diagonale c'è il valore del vertice corrispondente ES: in $A[i][i]$ c'è il valore di $\\alpha_i$)<br>\n",
    "    $W=(w_{ij})$ è la matrice dei pesi degli archi<br>\n",
    "    $D = diag(\\sum_{i \\neq j} W_{ij})$ è la matrice diagonale in cui in ogni casella diagonale c'è la somma dei pesi degli archi che entrano nel vertice corrispondente (ES: in $D[i][i]$ c'è la somma dei pesi degli archi che entrano nel vertice $i$).<br><br>\n",
    "    Se $A$ è l'identità e ho solo $D-W$ allora si chiama ***Laplaciano non normalizzato***.<br>\n",
    "    Se $A=D$ si chiama laplaciano di tipo ***random walk***.<br>\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Laplacians\n",
    "\n",
    "Siccome il laplaciano è una matrice simmetrica, perché $W$ è simmetrica, e $D$ è diagonale ed è semidefinita positiva, ammette $n$ autovalori e autovettori. Sono reali e ortogonali tra di loro, ma non è detto che siano distinti: posso avere anche autovalori a 0, basta che non siano negativi, perché se l'autovalore di una matrice è negativo il suo autovettore è complesso. In questo caso, siccome la matrice è semidefinita positiva e ha determinante maggiore di 0 gli autovalori sono tutti positivi o nulli e gli autovettori sono tutti reali e ortogonali tra di loro.\n",
    "\n",
    "#### Prodotto scalare tra il gradiente:<br>\n",
    "\n",
    "### $E_{Dir}(f) = \\frac{1}{2} 〈\\triangledown f, \\triangledown f〉_{L^2(\\epsilon)} = 〈\\Delta f, f〉_{L^2(V)}$\n",
    "\n",
    "dove $L^2(\\epsilon)$ indica la norma al quadrato nello spazio degli archi $=$ \"per tutti gli archi\"\n",
    "\n",
    "Il prodotto scalare del gradiente fatto per tutti gli archi (la somma del gradiente al quadrato per ogni arco) si chiama <span style=\"color:gold;\">Energia di Dirichlet</span> ed è uguale al prodotto scalare tra il laplaciano e il campo:\n",
    "\n",
    "### $E_{Dir}(f) = f^T \\Delta f$\n",
    "\n",
    "dove $f$ è il mio campo di cui sto facendo il gradiente trasposto per il laplaciano per f di nuovo = la norma al quadrato del gradiente su se stesso.\n",
    "\n",
    "La norma al quadrato del gradiente misura la ***smoothness*** di una funzione, ovvero quanto la funzione è \"morbida\" e non ha delle discontinuità. L'energia di Dirichlet è alta quando la mia funzione è frastagliata e discontinua, ed è bassa quando la mia funzione è liscia e continua, ovvero il gradiente si avvicina a zero.\n",
    "\n",
    "Un grafo è \"piatto\" se il valore che c'è sugli archi è basso, oppure se c'è un valore su due archi molto alto e $f_i, f_j$ sono uguali.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smoothest Orthobasis of a graph\n",
    "\n",
    "Vogliamo trovare un set di vettori ortogonali tra di loro tali per cui l'energia di Dirichlet sia minima, ovvero voglio i campi tali per cui f trasposto per il laplaciano per f sia minima ($f^T \\Delta f$).\n",
    "\n",
    "Ciò significa trovare i vertici del grafo che sono collegati tra di loro nel modo più \"smooth\" possibile.\n",
    "\n",
    "Quello che sto facendo è partizionare il grafo in più gruppi più omogenei possibili tra di loro &rarr; trova i set di vertici che hanno i pesi degli archi e i valori dei campi più vicini tra di loro.\n",
    "\n",
    "La soluzione di questo problema sono i primi n autovettori del laplaciano &rarr; sono $n$ possibili campi che minimizzano l'energia di Dirichlet.\n",
    "\n",
    "Gli autovettori del laplaciano mi danno delle partizioni del grafo che sono a bassa energia di Dirichlet, cioè all'interno della partizione i vertici sono molto simili tra di loro.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering con i grafi\n",
    "\n",
    "Definito un grafo con gli elementi sui vertici e sugli archi la similarità tra gli elementi, e una funzione di similarità\n",
    "\n",
    "### $w_{ij} = f(V_i, V_j)$\n",
    "\n",
    "Clusterizzare significa trovare una partizione di questo grafo in gruppi, in cui all'interno di un gruppo gli elementi siano molto simili tra di loro e tra gruppi diversi gli elementi siano molto dissimili.\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering12](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering12.png)\n",
    "\n",
    "Se devo partizionare questo grafo in due gruppi ho bisogno di una funzione obiettivo:\n",
    "\n",
    "### $CUT(A,B) = \\sum_{i \\in A, j \\in B} w_{ij}$\n",
    "\n",
    "Tramite il taglio di un grafo voglio eliminare degli archi e quindi minimizzare la somma dei pesi degli archi che collegano due gruppi diversi.\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering13](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering13.png)\n",
    "\n",
    "Il valore del taglio è equivalente alla somma del valore degli archi tagliati:\n",
    "\n",
    "&rarr; $cut(A,B) = 0.3$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Objectives\n",
    "\n",
    "Siccome sugli archi è stata messa la similarità tra gli elementi, voglio trovare il taglio che ha valore minimo: <span style=\"color:gold;\">Minimum Cut</span> &rarr; $min cut(A,B)$\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
