{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Consente di raggruppare elementi simili in gruppi chiamati cluster. Per poter raggrupparli è necessario che ci sia un'alta similiarità tra gli elementi della stessa classe e una basse similiarità tra gli elementi di classi diverse.\n",
    "\n",
    "In fase di design è necessario scegliere le feature su cui fare clustering e la funzione di similiarità.\n",
    "\n",
    "La similiarità deve soddifare le seguenti proprietà:\n",
    "- Simmetria D(A, B) = D(B, A)\n",
    "- Costanza della similiarità D(A, A) = 0\n",
    "- Positività D(A, B) = 0 se e solo se A = B\n",
    "- Disuguaglianza triangolare D(A, B) ≤ D(A, C) + D(B, C)\n",
    "\n",
    "Utilizzando il piano euclideo, la distanza euclidea è a tutti gli effetti una misura della similiarità.\n",
    "\n",
    "There are two types of Clustering\n",
    "Partitional algorithms: Construct various partitions and then evaluate them by\n",
    "some criterion\n",
    "Hierarchical algorithms: Create a hierarchical decomposition of the set of objects\n",
    "using some criterion\n",
    "\n",
    "\n",
    "Per fare clustering ci sono due strategie:\n",
    "- Partizionale: tipo di clustering iterativo in cui si costruiscono varie partizioni e poi si valutano con un criterio in modo gerarchico. Viene costruito un albero di merge o split.\n",
    "- Gerarchico: in un unico step divide già tutti gli elementi nei gruppi, creandoa una decomposizione gerarchica dell'insieme di oggetti usando un criterio.\n",
    "\n",
    "Qualunque sia la modalità di clustering le proprietà richieste a un algoritmo di clustering sono:\n",
    "- Scalabilità\n",
    "- Capacità di gestire di dati diversi\n",
    "- Cercare di avere minima conoscenza del dominio possibile (come prerequisito dovrei sapere il meno possibile dei dati che sto trattando)\n",
    "- Deve essere in grado di gestire il rumore e gli outlier, cioè i dati che non appartengono a nessun cluster\n",
    "- Non devono dipendere dall'ordine in cui si presentano i dati\n",
    "- Devono poter incorporare dei vincoli\n",
    "- Deve essere interpretabile\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "Dal momento che non possiamo testare tutti gli alberi possibili dobbiamo fare una ricerca euristica di tutti gli alberi possibili. Possiamo farlo in due modi:\n",
    "- ***Bottom-Up*** (agglomerativo): partendo da ogni elemento in un proprio cluster, trova la migliore coppia da unire in un nuovo cluster. Ripeti fino a quando tutti i cluster sono uniti. \n",
    "- ***Top-Down*** (divisivo): partendo da tutti i dati in un unico cluster, considera ogni possibile modo di dividere il cluster in due. Scegli la migliore divisione e operare ricorsivamente su entrambi i lati.\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering1](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering1.png)\n",
    "\n",
    "Se due elementi sono connessi alla stessa riga, allora appartengono allo stesso gruppo.\n",
    "\n",
    "Una volta costrutito l'albero o ***dendogramma***, per ottenere i cluster bisogna tagliare l'albero in un certo punto. Per fare ciò bisogna scegliere un valore di soglia.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start with a Distance or Similarity\n",
    "\n",
    "Per aggregare gli elementi in fase iniziale, si costruisce una matrice in cui nelle righe e nelle colonne ci sono gli elementi, mentre nella casella in corrispondenza di due elementi c'è la distanza o la similarità tra quegli elementi.\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering2](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering2.png)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottom-Up (agglomerativo)\n",
    "\n",
    "All'inizio parto con tutti gli elementi disgiunti e con una misura di distanza/similiarità tra gli elementi.\n",
    "\n",
    "Per tutte le possibili coppie dei miei elementi misuro la loro distanza e unisco i due elementi che hanno la distanza minore.\n",
    "\n",
    "### Tecniche di Linkage: strategie per il calcolo della distanza tra due cluster\n",
    "\n",
    "- **Single Linkage** (nearest neighbor): la distanza tra due cluster è determinata dalla distanza tra i due oggetti più vicini nei due cluster diversi (crea cluster molto elungati)\n",
    "- **Complete Linkage** (furthest neighbor): la distanza tra due cluster è determinata dalla distanza tra i due oggetti più lontani nei due cluster diversi (crea cluster molto compatti)\n",
    "- **Group Average Linkage**: la distanza tra due cluster è determinata dalla media delle distanze tra tutti gli oggetti nei due cluster diversi\n",
    "- **Wards Linkage**: (è la media diviso la varianza per la distanza e media per la varianza per la similiarità) &rarr; si cerca di minimizzare la varianza dei cluster uniti\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering3](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering3.png)\n",
    "\n",
    "Smetto di aggregare quando la distanza elemento-gruppo o gruppo-gruppo è sopra una certa soglia.\n",
    "\n",
    "Quindi il clustering gerarchico è accompagnato da due iperparametri:\n",
    "- la strategia di aggregazione\n",
    "- la soglia oltre la quale devo smettere di aggregare\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Riassumendo: hierarchical clustering\n",
    "\n",
    "- Non devo stabilire a priori il numero di cluster, però devo definire la soglia\n",
    "- La natura gerarchica è intuitiva per l'uomo &rarr; il dendogramma è facile da interpretare\n",
    "- Non scala bene: perché ogni volta devo calcolare la distanze con tutti gli altri elementi &rarr; $O(n^2)$, dove $n$ è il numero totale di elementi\n",
    "- Come tutti gli algoritmi di ricerca euristica, i locali ottimi sono un problema &rarr; non è detto che un determinato partizionamento sia il migliore a livello globale\n",
    "- L'interpretazione dei risultati è soggettiva\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partial clustering\n",
    "\n",
    "Nongerarchica, ogni istanza è inserita in uno dei K cluster non sovrapposti.\n",
    "\n",
    "Dal momento che viene restituito un solo insieme di cluster, l'utente deve normalmente inserire il numero desiderato di cluster K.\n",
    "\n",
    "Quindi è necessario avere il numero di cluster desiderati.\n",
    "\n",
    "# K-means\n",
    "\n",
    "Algoritmo di clustering che utilizza la funzione <span style=\"color:gold;\">Squared Error</span>\n",
    "\n",
    "Dato un set di punti ($x_1, x_2, ..., x_n$) e $k$ insiemi $S = \\{S_1, S_2, S_3, ..., S_k\\}$.\n",
    "\n",
    "La funzione obiettivo è:\n",
    "\n",
    "### $argmin_S \\sum_{i=1}^k \\sum_{x \\in S_i} ||x - \\mu_i||^2$\n",
    "\n",
    "dove $\\mu_i$ è la media dei punti in $S_i$.\n",
    "\n",
    "&rarr; ***Quindi cerco di minimizzare la distanza tra i punti e il centro del cluster.***\n",
    "\n",
    "La funzione obiettivo sopra è uguale a:\n",
    "\n",
    "### $argmin_S \\sum_{i=1}^k \\frac{1}{|S_i|} Var(S_i)$\n",
    "\n",
    "Questo è equivalente a:\n",
    "\n",
    "### $argmin_S \\sum_{i=1}^k \\sum_{x,y \\in S_i} ||x - y||^2$\n",
    "\n",
    "# Proof\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering4](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering4.png)\n",
    "\n",
    "Quindi voglio trovare le distanze minime per cui sommando tutte le distanze dal centro dei cluster rossi in figura ottengo la distanza minima.\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering5](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering5.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Algoritmo K-means Naive procedure\n",
    "\n",
    "\n",
    "Inizializza $k$ cluster centers randomici\n",
    "\n",
    "***while*** i centri non sono aggiornati o il numero massimo di iterazioni è stato raggiunto:\n",
    "\n",
    "- Assegna ogni punto al centro più vicino\n",
    "- Calcola per ogni cluster la media dei punti assegnati\n",
    "- $\\mu^{new}_{S_i} = \\Sigma{x \\in S_i} \\frac{x}{|S_i|}$\n",
    "- $\\mu_{S_i} = \\mu^{new}_{S_i}$\n",
    "\n",
    "***end***\n",
    "\n",
    "*A ogni update, aggiorno la posizione del centro cluster e il nuovo centro diventa la media delle coordinate (feature) dei punti che sono stati assegnati a quel cluster.*\n",
    "\n",
    "#### Iterazione 1: fase di assegnamento dei punti\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering6](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering6.png)\n",
    "\n",
    "#### Iterazione 2: assengno i punti al centro più vicino\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering7](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering7.png)\n",
    "\n",
    "#### Iterazione 3: il centro si sposta perché è uguale alla media di tutte le coordinate dei punti assegnati al cluster\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering8](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering8.png)\n",
    "\n",
    "#### Iterazione 4: nuova fase di asegnamento + update\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering9](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering9.png)\n",
    "\n",
    "Non è detto che scegliere i centi iniziali a caso portino sempre alla stessa soluzione! È bene sceglierli casualmente ma cercando di ricoprire tutto lo spazio (range min-max di ogni feature).\n",
    "\n",
    "Un'altra soluzione potrebbe essere lanciare tante volte l'algoritmo e scegliere la soluzione migliore, ovvero quelle che coinvolgono partizionamenti che capitano più spesso.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comments on kmeans\n",
    "\n",
    "Pro:\n",
    "- Relativamente efficiente: $O(tkn)$, dove $n$ è il numero di oggetti, $k$ è il numero di cluster e $t$ è il numero di iterazioni. Normalmente $k$ e $t$ sono molto più piccoli di $n$.\n",
    "- Spesso termina in un ottimo locale. L'ottimo globale può essere trovato utilizzando tecniche come il *deterministic annealing* e gli *algoritmi genetici*\n",
    "\n",
    "Contro:\n",
    "- **<u>Applicabile solo quando la media è definita, quindi cosa succede con i dati categorici?</u>** &rarr; non posso usare la media per i quei dati che possono essere solo booleani, ad esempio.\n",
    "- Bisogna specificare $k$, il numero di cluster, in anticipo\n",
    "- Non è in grado di gestire dati rumorosi e outlier\n",
    "- Non è adatto per scoprire cluster con forme non convesse\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Partitioning Around Medoids\n",
    "\n",
    "Per ovviare al problema del K-Means quando non posso fare la media.\n",
    "\n",
    "Se al posto di creare dei nuovi punti (la media) utilizzo dei punti che sono già all'interno del mio dataset, sono sicuro di rimanere nel \"dominio\" di questo problema.\n",
    "\n",
    "Trova gli oggetti rappresentativi, chiamati medoids, nei cluster.\n",
    "- PAM (Partitioning Around Medoids, 1987)\n",
    "- Parte da un set iniziale di medoidi e iterativamente sostituisce uno dei medoids con uno dei non-medoidi se migliora la distanza totale del clustering risultante\n",
    "- PAM funziona efficacemente per piccoli set di dati, ma non scala bene per grandi set di dati\n",
    "\n",
    "Algoritmo:\n",
    "1. Seleziona $k$ punti a caso del mio dataset e ogni elemento è assegnato al centro più vicino\n",
    "2. Il centro di un cluster è il punto che ha distanza minima da tutti gli altri punti del cluster &rarr; questo è il nuovo centro\n",
    "3. Reitero a convergenza come nel K-means\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se ho delle superfici di separazioni che sono degli sferoidi, il K-means non funziona bene perché non è in grado di gestire superfici non convesse.\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering10](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering10.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
