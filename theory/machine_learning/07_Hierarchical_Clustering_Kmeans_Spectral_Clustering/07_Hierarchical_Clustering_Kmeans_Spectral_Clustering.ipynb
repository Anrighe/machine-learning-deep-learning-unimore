{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Consente di raggruppare elementi simili in gruppi chiamati cluster. Per poter raggrupparli è necessario che ci sia un'alta similiarità tra gli elementi della stessa classe e una basse similiarità tra gli elementi di classi diverse.\n",
    "\n",
    "In fase di design è necessario scegliere le feature su cui fare clustering e la funzione di similiarità.\n",
    "\n",
    "La similiarità deve soddifare le seguenti proprietà:\n",
    "- Simmetria D(A, B) = D(B, A)\n",
    "- Costanza della similiarità D(A, A) = 0\n",
    "- Positività D(A, B) = 0 se e solo se A = B\n",
    "- Disuguaglianza triangolare D(A, B) ≤ D(A, C) + D(B, C)\n",
    "\n",
    "Utilizzando il piano euclideo, la distanza euclidea è a tutti gli effetti una misura della similiarità.\n",
    "\n",
    "There are two types of Clustering\n",
    "Partitional algorithms: Construct various partitions and then evaluate them by\n",
    "some criterion\n",
    "Hierarchical algorithms: Create a hierarchical decomposition of the set of objects\n",
    "using some criterion\n",
    "\n",
    "\n",
    "Per fare clustering ci sono due strategie:\n",
    "- Partizionale: tipo di clustering iterativo in cui si costruiscono varie partizioni e poi si valutano con un criterio in modo gerarchico. Viene costruito un albero di merge o split.\n",
    "- Gerarchico: in un unico step divide già tutti gli elementi nei gruppi, creandoa una decomposizione gerarchica dell'insieme di oggetti usando un criterio.\n",
    "\n",
    "Qualunque sia la modalità di clustering le proprietà richieste a un algoritmo di clustering sono:\n",
    "- Scalabilità\n",
    "- Capacità di gestire di dati diversi\n",
    "- Cercare di avere minima conoscenza del dominio possibile (come prerequisito dovrei sapere il meno possibile dei dati che sto trattando)\n",
    "- Deve essere in grado di gestire il rumore e gli outlier, cioè i dati che non appartengono a nessun cluster\n",
    "- Non devono dipendere dall'ordine in cui si presentano i dati\n",
    "- Devono poter incorporare dei vincoli\n",
    "- Deve essere interpretabile\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering\n",
    "\n",
    "Dal momento che non possiamo testare tutti gli alberi possibili dobbiamo fare una ricerca euristica di tutti gli alberi possibili. Possiamo farlo in due modi:\n",
    "- ***Bottom-Up*** (agglomerativo): partendo da ogni elemento in un proprio cluster, trova la migliore coppia da unire in un nuovo cluster. Ripeti fino a quando tutti i cluster sono uniti. \n",
    "- ***Top-Down*** (divisivo): partendo da tutti i dati in un unico cluster, considera ogni possibile modo di dividere il cluster in due. Scegli la migliore divisione e operare ricorsivamente su entrambi i lati.\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering1](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering1.png)\n",
    "\n",
    "Se due elementi sono connessi alla stessa riga, allora appartengono allo stesso gruppo.\n",
    "\n",
    "Una volta costrutito l'albero o ***dendogramma***, per ottenere i cluster bisogna tagliare l'albero in un certo punto. Per fare ciò bisogna scegliere un valore di soglia.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start with a Distance or Similarity\n",
    "\n",
    "Per aggregare gli elementi in fase iniziale, si costruisce una matrice in cui nelle righe e nelle colonne ci sono gli elementi, mentre nella casella in corrispondenza di due elementi c'è la distanza o la similarità tra quegli elementi.\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering2](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering2.png)\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bottom-Up (agglomerativo)\n",
    "\n",
    "All'inizio parto con tutti gli elementi disgiunti e con una misura di distanza/similiarità tra gli elementi.\n",
    "\n",
    "Per tutte le possibili coppie dei miei elementi misuro la loro distanza e unisco i due elementi che hanno la distanza minore.\n",
    "\n",
    "### Tecniche di Linkage: strategie per il calcolo della distanza tra due cluster\n",
    "\n",
    "- **Single Linkage** (nearest neighbor): la distanza tra due cluster è determinata dalla distanza tra i due oggetti più vicini nei due cluster diversi (crea cluster molto elungati)\n",
    "- **Complete Linkage** (furthest neighbor): la distanza tra due cluster è determinata dalla distanza tra i due oggetti più lontani nei due cluster diversi (crea cluster molto compatti)\n",
    "- **Group Average Linkage**: la distanza tra due cluster è determinata dalla media delle distanze tra tutti gli oggetti nei due cluster diversi\n",
    "- **Wards Linkage**: (è la media diviso la varianza per la distanza e media per la varianza per la similiarità) &rarr; si cerca di minimizzare la varianza dei cluster uniti\n",
    "\n",
    "![07_Hierarchical_Clustering_Kmeans_Spectral_Clustering3](images/07_Hierarchical_Clustering_Kmeans_Spectral_Clustering3.png)\n",
    "\n",
    "Smetto di aggregare quando la distanza elemento-gruppo o gruppo-gruppo è sopra una certa soglia.\n",
    "\n",
    "Quindi il clustering gerarchico è accompagnato da due iperparametri:\n",
    "- la strategia di aggregazione\n",
    "- la soglia oltre la quale devo smettere di aggregare"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
