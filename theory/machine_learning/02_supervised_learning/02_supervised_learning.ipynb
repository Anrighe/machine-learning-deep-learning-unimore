{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data**: a set of data records, where every record has k dimensions, k features\n",
    "**Class**: used to label each example (esempio con 2 classi: stabilire se una persona √® maschio/femmina)\n",
    "\n",
    "In order to learn we need to:\n",
    "- **Train**: by using the training data\n",
    "- **Test**: by testing the model using **unseen** test data \n",
    "\n",
    "**Accuracy**: $\\frac{\\text{Number of total correct classification}}{\\text{Total number of cases}}$\n",
    "\n",
    "---\n",
    "\n",
    "# Training pipeline\n",
    "![training-pipeline](images/supervised_learning1.png)\n",
    "\n",
    "The function used in the **learning model** uses:\n",
    "- **ùë•**: data\n",
    "- **Œò**: parameters\n",
    "- **Œì**: hyperparameters\n",
    "\n",
    "**L**: is the **Loss function**, and measures the predicted class of the classifies compared to my predicion. It enables me to measure the error so that I can tweak the parameters until convergence\n",
    "\n",
    "---\n",
    "\n",
    "# Inference pipeline\n",
    "\n",
    "![training-pipeline](images/supervised_learning2.png)\n",
    "\n",
    "An **inference pipeline** is a series of steps or processes that are used to make predictions or inferences on new data using a trained machine learning model.\n",
    "\n",
    "---\n",
    "\n",
    "# Definition of learning:\n",
    "Given a Dataset **D** a Class task **C** a performance function **L** a computer system is said to learn the task **C** from data **D** if after osberving **D** performance on **L** improves at a level that is superior with respect to random guessing.\n",
    "\n",
    "# Fundamental assumption:\n",
    "Learning is possible only when training and inference are performed from data that comes from the same distribution.\n",
    "Samples must be independently and identically distributed ***(i.i.d.)*** from training and testing.\n",
    "\n",
    "Machine learning works only when training set and test set are iid, which means the performance must stay the same on all types of sample.\n",
    "\n",
    "---\n",
    "\n",
    "Con **ùîº** si intende il valor atteso (quando si √® in una distribuzione equiprobabilbe √® il valor medio), ovvero una variabile aleatoria (*x*) moltiplicata per la sua probabilit√† di esistere (di assumere un certo valore) integrata su tutti i possibili valori che pu√≤ assumere:\n",
    "\n",
    "![expected-value](images/supervised_learning7.png)\n",
    "\n",
    "---\n",
    "\n",
    "# i.i.d. revised\n",
    "Given:\n",
    "- f: Classification function\n",
    "- c: Class task\n",
    "- D: data distribution\n",
    "\n",
    "the <span style=\"color:gold;\">**risk**</span> of the classifier is:\n",
    "\n",
    "\n",
    "![risk](images/supervised_learning3.png)\n",
    "\n",
    "Il rischio √® il valore atteso di tutte le volte che la funzione di classificazione sbaglia a predirre la classe (√® praticamente l'errore medio).\n",
    "Nella formula sopra, non √® misurabile perch√© non si conosce **D**, tuttavia si pu√≤ stimare con l'<span style=\"color:gold;\">**empirical risk**</span> usando:\n",
    "- un insieme **S** di *m* elementi pescati dalla distribuzione **D** (i campioni)\n",
    "\n",
    "\n",
    "![empirical-risk](images/supervised_learning4.png)\n",
    "\n",
    "La formula sopra indica la media ($\\frac{1}{m}$), dove conto 1 ogni volta che *f*(*x<sub>i</sub>*) ‚â† *c*(*x<sub>i</sub>*), ovvero ogni volta che sbaglio a predirre la classe.\n",
    "\n",
    "Idealmente vorrei che il rischio generale fosse il pi√π uguale possibile all'errore empirico.\n",
    "La differenza tra i due √® chiamata <span style=\"color:gold;\">**generalization error**</span>, e quantifica la differenza in performance tra il training e il testing.\n",
    "\n",
    "Quindi, se **S** √® stato campionato in modo i.i.d. dalla distribuzione **D**, e sono tutti distribuiti allo stesso modo, allora il valor medio √® uguale per tutti gli **m** elementi. \n",
    "\n",
    "Partiziono il mio insieme **D** in **m** sottoinsiemi **S**, faccio l'errore medio su ogni sottoinsieme **S**, il quale per via del fatto che i dati sono i.i.d, sar√† uguale all'errore medio sul singolo insieme **S**.\n",
    "\n",
    "![iid-case1](images/supervised_learning5.png)\n",
    "\n",
    "Quindi prendendo misurazioni i.i.d., a livello teorico, misurare l'errore solo sul mio insieme **S** (sul mio dataset) sarebbe uguale a misurarlo su tutta la distribuzione **D**.\n",
    "\n",
    "Se i campioni sono i.i.d. allora il valore atteso assume lo stesso valore in ogni sottoinsieme di campioni che vengono campionati dalla distribuzione principale:\n",
    "\n",
    "![iid-case2](images/supervised_learning6.png)\n",
    "\n",
    "---\n",
    "\n",
    "# Misurare le performance\n",
    "\n",
    "- **Predictive accuracy** = $\\frac{\\text{Number of correct classifications}}{\\text{Total number of test cases}}$\n",
    "- **Efficiency**: \n",
    "    - time to construct the model\n",
    "    - time to use the model\n",
    "- **Robustness**: capacit√† di accettare ed elaborare dati che sono soggetti a rumore\n",
    "- **Scalability**: efficiency in disk-resident databases\n",
    "- **Interpretability**: dove un modello √® interpretabile quando riesco a capire quali sono i processi che l'hanno portato a una particolare decisione\n",
    "- **Compactness of the model**: size of the tree, or the number of rules.\n",
    "\n",
    "---\n",
    "\n",
    "Il test set √® anche chiamato <span style=\"color:gold;\">**holdout set**</span> e non deve mai essere lo stesso del training set.\n",
    "\n",
    "---\n",
    "\n",
    "# Hyperparameters\n",
    "Sono quei parametri che non sono appresi dal modello, ma che sono fissati a priori (es. il numero di layer, o dei weights).\n",
    "\n",
    "Il **validation set** √® un altro sottoinsieme del dataset **D** che viene usato per trovare i valori ottimali degli hyperparameters.\n",
    "\n",
    "---\n",
    "\n",
    "# Train-Validation-Test\n",
    "- Divido il dataset D in 3 sottoinsiemi:\n",
    "    - ***T<sub>r</sub>***: training set\n",
    "    - ***V***: validation set\n",
    "    - ***T<sub>e</sub>***: test set\n",
    "- Il modelli vengono addestrati sul training set ***T<sub>r</sub>*** per ogni set di iperparametri che sto valutando\n",
    "- Misuro il **validation error** ***Val<sub>i</sub>*** di ogni modello valutato sul validation set ***V***\n",
    "- Seleziono gli iperparametri con il **validation error** pi√π basso e il classificatore viene allenato nuovamente utilizzando questi nuovi iperparametri sul training set ***T<sub>r</sub>*** + il validation set ***V*** \n",
    "- ottengo il modello finale del quale posso misurare la performance sul test set ***T<sub>e</sub>***\n",
    "\n",
    "---\n",
    "\n",
    "# k-fold Cross-validation\n",
    "\n",
    "- Divido randomicamente il dataset **D** in un insieme di **K** blocchi **B<sub>1</sub>, B<sub>2</sub>, ..., B<sub>K</sub>**\n",
    "- Per ogni k = 1, ..., K:\n",
    "    - Addestro il modello su tutto il dataset tranne il blocco k-esimo: **D** - **B<sub>k</sub>**\n",
    "    - Valuto il modello con i test sul blocco k-esimo **B<sub>k</sub>**\n",
    "- Questa procedura fornisce **K** accuracies\n",
    "- L'accuratezza del mio modello √® data dalla media di tutte le **K** accuracies\n",
    "\n",
    "---\n",
    "\n",
    "# Confusion matrix\n",
    "\n",
    "- **True Positive**: se io ho predetto correttamente la classe positiva (se io ho predetto incendio e c'√® un incendio)\n",
    "- **False Positive**: Se non ho predetto correttamente la classe positiva (se io ho detto incendio ma l'incendio NON c'√®)\n",
    "- **True Negative**: se io ho predetto correttamente la classe negativa (se io ho detto che NON c'√® un incendio e NON c'√® un incendio)\n",
    "- **False Negative**: se io non ho predetto correttamente la classe negativa (se io ho detto che NON c'√® un incendio ma c'√® un incendio)\n",
    "\n",
    "---\n",
    "\n",
    "# Precision e Recall\n",
    "\n",
    "- **Precision**: $\\frac{\\text{True Positive}}{\\text{True Positive + False Positive}}$\n",
    "    - La Precision √® il numero degli esempi positivi correttamente classificati diviso il numero totale degli esempi classificati come positivi. Essa √® una misura di accuratezza quando il sistema si esprime. <u>Ci√≤ non implica che il sistema si esprima spesso</u>.\n",
    "\n",
    "- **Recall**: $\\frac{\\text{True Positive}}{\\text{True Positive + False Negative}}$\n",
    "    - La Recall √® il numero degli esempi positivi correttamente classificati diviso il numero totale degli esempi positivi. La recall √® una misura di copertura, ovvero quando si √® espresso il sistema. <u>Ci√≤ non implica che il sistema si esprima correttamente</u>.\n",
    "\n",
    "- **F1-score**: $\\frac{2 \\cdot \\text{Precision} \\cdot \\text{Recall}}{\\text{Precision + Recall}}$\n",
    "    - L'F1-score √® la media armonica tra Precision e Recall. \"Pesa\" di pi√π il valore pi√π basso tra i due.\n",
    "\n",
    "---\n",
    "\n",
    "# ROC curve\n",
    "\n",
    "I valori di precision e recall si possono rappresentare come una curva. Con essi pu√≤ essere usata una soglia per far attivare il mio sistema di classificazione, e in base alla soglia usata avr√≤ delle modifiche sui valori di precision e recall.\n",
    "\n",
    "La ROC Curve viene usata per studiare quale potrebbe essere il valore ottimo di una soglia per poter avere precision e recall pi√π alti possibile\n",
    "\n",
    "![roc-curve](images/supervised_learning8.png)\n",
    "\n",
    "---\n",
    "\n",
    "# Calcolo performance con output di tipo statistico:\n",
    "Posso misurare le performance di un modello che mi restituisce un output di tipo statistico in due modi:\n",
    "1. Posso mettere una soglia, e converto l'output in \"categorico\" (es: se √® sopra a 0.9 incendio si, e se √® sotto incendio no)\n",
    "2. <u>Misuro la distanza tra due distribuzioni</u>:\n",
    "    - **Coefficiente di Bhattacharyya**: misura approssimativa della sovrapposizione tra due campioni statistici. Date due distribuzioni **P** e **Q**, √® l'integrale del loro prodotto:\n",
    "        \n",
    "        $BC(P,Q) = \\int_x P(x)Q(x)dx$\n",
    "\n",
    "    - **KL Divergence**: misura non simmetrica dell'informazione persa quando la distribuzione **Q** viene usata per approssimare la distribuzione **P** (\"P dato Q\"). Si tratta dell'integrale del rapporto tra le due distribuzioni. Se **Q** √® uguale a **P** allora la divergenza √® 0:\n",
    "        \n",
    "        $KL(P|Q) = \\int_x P(x) \\log \\frac{P(x)}{Q(x)}dx$\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "# Cross Entropy\n",
    "\n",
    "La cross entropy valuta il numero medio di bit per scoprire un determinato valore codificato tramite una distribuzione **P** quando la distribuzione di origine (quella a cui apparteneva quel dato) √® **P**, ovvero quanti bit mi servono per scoprire che dato √® se sto sbagliando a modellare la distribuzione da cui quel dato deve venire (se sto usando un'approssimazione).\n",
    "\n",
    "**P** √® la distribuzione che approssima **P**, che potrebbe essere la distribuzione che il classificatore stima per un evento.\n",
    "\n",
    "L'**entropia** misura <u>quanto sia casuale o incerta la distribuzione delle classificazioni nei dati</u>. Un dataset con entropia alta contiene dati in cui le classificazioni sono distribuite in modo caotico e uniforme tra le diverse categorie. In altre parole, non c'√® un modello o una tendenza chiara nei dati.\n",
    "\n",
    "D'altra parte, un dataset con entropia bassa contiene dati in cui le classificazioni sono molto coerenti e omogenee. Questo significa che i dati tendono a raggrupparsi in categorie specifiche, rendendo pi√π facile per un modello di apprendimento automatico effettuare previsioni accurate.\n",
    "L'**entropia** si calcola tramite:\n",
    "    $H(P) = - \\int_x P(x) \\log P(x)dx$\n",
    "\n",
    "La **cross entropy** tra due distribuzioni **P** e **Q** √® l'entropia di P $H(P)$ + la KL divergence di **P** dato **Q** $KL(P|Q)$:\n",
    "    $H(P,Q) = H(P) + KL(P|Q)$\n",
    "\n",
    "Dimostrazione del perch√© la cross entropu √® uguale a quello che √®:\n",
    "\n",
    "![cross-entropy](images/supervised_learning9.png)\n",
    "\n",
    "---\n",
    "\n",
    "# Teorema di Kraft-McMillan\n",
    "Un certo valore *x<sub>i</sub>* pu√≤ essere identificato con *l<sub>i</sub>* bit con probabilit√† $Q(x_i) = 2^{-l_i}$, dove *l<sub>i</sub>* √® la lunghezza della codifica di *x<sub>i</sub>*.\n",
    "\n",
    "Se si considera la quantit√† $Q(x_i)$ e calcolo il valore atteso di ***l*** rispetto a ***P(x)***, ovvero voglio calcolare il numero di bit che dovrei usare per rappresentare ***P(x)*** con $Q(x_i)$, dovrei fare (in caso discreto uso la sommatoria):\n",
    "\n",
    "\n",
    "![cross-entropy-kraft-mcmillan](images/supervised_learning10.png)\n",
    "\n",
    "Ovvero il valore attesto di ***l*** rispetto a ***P***, che √® uguale alla sommatoria di ***P(x)*** * ***l***, ma ***l***, dalla formula $Q(x_i)= 2^{-l_i}$, √® uguale a $-log_2 Q(x_i)$, che per propriet√† dei logaritmi diventa $log_2 Q(x_i)^{-1}$.\n",
    "\n",
    "Quindi, il valore atteso di bit che mi servirebbero per rappresentare **x** con **Q** al posto che con **P** √® uguale a:\n",
    "\n",
    "$\\sum_x P(x) log_2 Q(x_i)^{-1}$\n",
    "\n",
    "E se porto fuori il meno, diventa:\n",
    "\n",
    "$- \\sum_x P(x) log_2 Q(x_i)$\n",
    "\n",
    "Che √® la stessa formula del calcolo della Cross Entropy con la differenza che qua sono nel caso discreto (sommatoria) e l√† sono nel continuo (integrale).\n",
    "\n",
    "---\n",
    "\n",
    "# Cross Entropy pt. 2\n",
    "La Cross Entropy pu√≤ essere usata come misura della classificazione dell'errore:\n",
    "Considero ***P*** come una distribuzione discreta con ***k*** possibili valori e il problema come una classificazione in ***k*** classi.\n",
    "- $P_i(x) = 1$ se ***x*** √® della classe ***i***\n",
    "- $Q_i(x)$ √® il punteggio di probabilit√† che il classificatore attribuisce alla classe ***i*** per l'elemento ***x***\n",
    "Fissato un dataset di ***n*** elementi equiprobabili, la Cross Entropy tra ***P*** e ***Q*** √® una misura di accuratezza del mio classificatore:\n",
    "\n",
    "![cross-entropy2](images/supervised_learning11.png)\n",
    "\n",
    "Sostanzialmente vuol dire che se $P_i(x)$ vale 1, io voglio che $Q_i(x) sia pi√π alta possibile. \n",
    "\n",
    "$P_i(x)$ funziona da abilitatore e sceglie quale valore di $Q_i(x)$ deve considerare.\n",
    "Tipicamente la Cross entropy si minimizza per via del $-$ davanti.\n",
    "Se ***Q*** √® alta per la classe corretta vuol necessariamente dire che √® bassa per tutte le altre classi perch√© le probabilit√† su i sommano a 1.\n",
    "\n",
    "#### Caso binario:\n",
    "Se ho solo due classi, la Cross Entropy si riduce a:\n",
    "\n",
    "![cross-entropy3](images/supervised_learning12.png)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
